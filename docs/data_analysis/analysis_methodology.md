# üìä Data Analytics Project Methodology Guide

*Complete workflow guide for data analytics projects using Python and SQL*

Este guia √© agn√≥stico aos dados e foca em passos reproduz√≠veis, melhores pr√°ticas e decis√µes claras para quem est√° a come√ßar em Data Analytics.

## üéØ **Workflow Sequencial (CRISP-DM adaptado)**

0. [Project Setup](#0-project-setup) - Configura√ß√£o inicial reproduz√≠vel
1. [Initial Data Overview](#1-initial-data-overview) - Invent√°rio r√°pido dos dados
2. [Business Understanding](#2-business-understanding) - Alinhamento de objetivos
3. [Data Understanding](#3-data-understanding) - An√°lise t√©cnica detalhada
4. [Database Design & Integration](#4-database-design-integration-planning) - Estrutura de dados
5. [Data Preparation](#5-data-preparation) - Limpeza e transforma√ß√£o
6. [Exploratory Data Analysis](#6-exploratory-data-analysis-eda) - Gera√ß√£o de insights
7. [Insights & Reporting](#7-insights-reporting) - Decis√µes e a√ß√µes

---

## 0) Project Setup
**Objetivo:** garantir reprodutibilidade, organiza√ß√£o e seguran√ßa desde o in√≠cio.  
**Entreg√°veis:** reposit√≥rio versionado, estrutura de pastas, ambiente configurado.

### ‚úÖ Checklist

- [ ] **Criar reposit√≥rio Git** e estrutura m√≠nima:
  ```text
  data-analytics-project/
  ‚îú‚îÄ .venv/
  ‚îú‚îÄ data/              # raw/, interim/, processed/
  ‚îÇ  ‚îú‚îÄ raw/
  ‚îÇ  ‚îú‚îÄ interim/
  ‚îÇ  ‚îî‚îÄ processed/
  ‚îú‚îÄ notebooks/         # 01_xxx.ipynb, 02_xxx.ipynb...
  ‚îú‚îÄ scripts/
  ‚îú‚îÄ sql/               # 01_exploration.sql, 02_cleaning.sql...
  ‚îú‚îÄ src/               # utils.py, pipelines/
  ‚îú‚îÄ reports/           # figures/, dashboards/
  ‚îú‚îÄ config/            # .env.example, params.yml
  ‚îú‚îÄ .env
  ‚îú‚îÄ .gitignore
  ‚îú‚îÄ main.py
  ‚îú‚îÄ pyproject.toml     # generated by UV
  ‚îú‚îÄ README.md
  ‚îî‚îÄ LICENSE
  ```

- [ ] **Definir ambiente Python** (preferencialmente UV) e depend√™ncias:
  ```bash
  # Setup with UV (recommended)
  uv init --python 3.11
  uv add pandas numpy matplotlib seaborn plotly
  uv add jupyter sqlalchemy psycopg2-binary
  uv add scikit-learn scipy statsmodels
  ```

- [ ] **Criar `.env.example`** (sem secrets) e validar vari√°veis
- [ ] **Definir conven√ß√µes:** nomes de ficheiros, prefixos de notebooks, commits

---

## 1) Initial Data Overview
**Objetivo:** invent√°rio r√°pido dos datasets dispon√≠veis para perceber o que temos antes de formular quest√µes de neg√≥cio.  
**Entreg√°veis:** invent√°rio de datasets, avalia√ß√£o inicial de qualidade, mapeamento de relacionamentos.

### 1.1) Dataset Inventory

- [ ] **Listar todos os ficheiros/tabelas** com:
  - Nome, tamanho (linhas √ó colunas), formato
  - Data da √∫ltima atualiza√ß√£o e fonte/origem
  - Respons√°vel pelos dados

```python
# Comandos essenciais para invent√°rio
df.shape                    # Dimens√µes (linhas, colunas)
df.info()                   # Tipos de dados e uso de mem√≥ria
df.head(10), df.tail(10)    # Primeiras/√∫ltimas linhas
df.columns.tolist()         # Lista completa de colunas
```

### 1.2) High-Level Quality Check

- [ ] **Valores em falta:** `df.isnull().sum()` e `df.isnull().mean() * 100`
- [ ] **Duplicados:** `df.duplicated().sum()` e por chave de neg√≥cio
- [ ] **Cardinalidade:** `df.nunique()` e ratio de unicidade
- [ ] **Tipos de dados:** validar se est√£o corretos (`df.dtypes`)
- [ ] **Valores extremos:** `df.describe()` para identificar problemas √≥bvios

### 1.3) Initial Relationship Identification

- [ ] **Vari√°veis comuns** entre datasets (nomes iguais/similares)
- [ ] **Potenciais chaves de join** (foreign keys)
- [ ] **Cardinalidade de relacionamentos** (1:1, 1:N, N:N)
- [ ] **Identificar dataset principal** (fact table) vs auxiliares (dimensions)

---

## 2) Business Understanding
**Objetivo:** alinhar problema, valor esperado e crit√©rios de sucesso baseados nos dados dispon√≠veis.  
**Entreg√°veis:** problem statement, hip√≥teses, KPIs, crit√©rios de aceita√ß√£o.

### Key Questions
- Que decis√£o de neg√≥cio precisa de ser tomada e por quem?
- Que KPIs/OKRs s√£o afetados? Qual √© a baseline e target?
- Que hip√≥teses priorit√°rias devemos testar?
- Que quest√µes espec√≠ficas queremos fazer aos dados? Com que prop√≥sito?

### ‚úÖ Checklist

- [ ] **Definir Problem Statement** (1‚Äì3 frases claras)
- [ ] **Mapear KPIs principais** e m√©tricas derivadas
- [ ] **Listar hip√≥teses** e impacto esperado (‚Üë/‚Üì KPI)
- [ ] **Crit√©rios de sucesso/falha** e decis√£o alvo
- [ ] **Formular quest√µes espec√≠ficas de neg√≥cio** para os dados
- [ ] **Definir constraints** (tempo, compliance, qualidade m√≠nima)

---

## 3) Data Understanding
**Objetivo:** an√°lise t√©cnica detalhada da qualidade, estrutura e conte√∫do dos dados usando ferramentas Python/SQL.  
**Entreg√°veis:** relat√≥rio de data understanding, dicion√°rio de dados, problemas e oportunidades identificados.

### 3.1) Metadata and Data Origin

- [ ] **Documentar fontes de dados:**
  - Data owner/pessoa respons√°vel
  - Data de publica√ß√£o/cria√ß√£o
  - Frequ√™ncia de atualiza√ß√£o (SLA)
  - Processo de gera√ß√£o dos dados
  - Depend√™ncias e sistemas upstream

### 3.2) Structure and Dimensions

- [ ] **An√°lise dimensional b√°sica:**
  - `df.shape` - Quantas linhas √ó colunas?
  - `df.info()` - Tipos de dados e uso de mem√≥ria
  - `df.columns.tolist()` - Lista completa de colunas
- [ ] **Valida√ß√£o de nomenclatura:**
  - Os t√≠tulos das colunas fazem sentido?
  - Conven√ß√µes consistentes?
  - Necess√°rio renomear colunas?

### 3.3) Data Types and Formats

- [ ] **Verificar tipos de dados** (`df.dtypes`):
  - Num√©ricos: int vs float, precis√£o adequada?
  - Strings: object vs category para otimiza√ß√£o?
  - Datas: datetime vs object string?
  - Booleanos: bool vs int/string?
- [ ] **Validar formatos espec√≠ficos:**
  - Datas: formato consistente, timezone
  - N√∫meros: separadores decimais, unidades
  - Texto: encoding, case sensitivity
  - IDs: padr√£o, comprimento fixo/vari√°vel

### 3.4) Content Analysis

- [ ] **Unicidade e cardinalidade** por coluna:
  - `df.nunique()` - Quantos valores √∫nicos?
  - `df.nunique() / len(df)` - Ratio de unicidade
  - Identificar candidatos a primary key
  - Identificar colunas categ√≥ricas (baixa cardinalidade)
- [ ] **Classifica√ß√£o estat√≠stica** de cada coluna:
  - Num√©rica cont√≠nua vs discreta
  - Categ√≥rica nominal vs ordinal
  - Temporal: ponto vs intervalo
  - Identificadores/chaves

### 3.5) Data Quality Assessment

- [ ] **An√°lise de valores em falta** (`df.isnull().sum()`):
  - Padr√£o missing: aleat√≥rio vs sistem√°tico?
  - % missing por coluna: `df.isnull().mean()`
  - Valores missing representam "zero" ou "desconhecido"?
  - Decidir estrat√©gia: remover, imputar ou manter como categoria
- [ ] **Detec√ß√£o preliminar de outliers:**
  - `df.describe()` - min/max suspeitos?
  - Valores imposs√≠veis (idades negativas, datas futuras)
  - M√©todo IQR: valores fora Q1-1.5√óIQR ou Q3+1.5√óIQR

### 3.6) Duplicates and Consistency

- [ ] **An√°lise de duplicados:**
  - `df.duplicated().sum()` - linhas 100% duplicadas
  - Duplicados por chave de neg√≥cio (`df.duplicated(subset=['key']).sum()`)
  - Investigar se duplicados s√£o leg√≠timos ou erros
- [ ] **Consist√™ncia interna:**
  - Campos calculados batem com componentes? (ex: total = sum(parts))
  - Relacionamentos l√≥gicos s√£o v√°lidos? (ex: end_date > start_date)
  - Intervalos esperados s√£o respeitados?

---

## 4) Database Design & Integration Planning
**Objetivo:** desenhar estrutura de dados coerente e estrat√©gia de integra√ß√£o antes da prepara√ß√£o final.  
**Entreg√°veis:** diagrama ER, estrat√©gia de integra√ß√£o, plano de joins, dicion√°rio de dados.

### 4.1) Entity and Attribute Mapping

- [ ] **Mapear entidades e atributos:**
  - Identificar fact tables (eventos, m√©tricas) vs dimension tables (refer√™ncias, categorias)
  - Documentar cada atributo (nome, tipo, descri√ß√£o, fonte)
- [ ] **Definir primary e foreign keys:**
  - Validar unicidade: `df.groupby(key).size().max() == 1`
  - Testar cobertura entre tabelas
- [ ] **Modelar relacionamentos:**
  - 1:1, 1:N, N:N (usar bridge tables se necess√°rio)
  - Cardinalidade esperada
  - Documentar regras de integridade referencial

### 4.2) Integration Strategy

- [ ] **Planeamento de joins:**
  - Ordem de joins (fact table como base)
  - Tipos de join (inner/left/outer) baseados em regras de neg√≥cio
  - Estrat√©gia para registos √≥rf√£os
- [ ] **Lidar com conflitos de nomenclatura:**
  - Colunas com mesmo nome: usar sufixos expl√≠citos
  - Valores conflituosos: definir source of truth (SSOT)
- [ ] **Criar diagrama ER simples** (dbdiagram.io, draw.io ou Mermaid)

```sql
-- Valida√ß√£o de foreign key antes dos joins
SELECT 
    COUNT(*) as total_records,
    COUNT(t2.id) as matched_records,
    COUNT(*) - COUNT(t2.id) as orphan_records,
    ROUND(100.0 * COUNT(t2.id) / COUNT(*), 2) as match_rate
FROM table1 t1
LEFT JOIN table2 t2 ON t1.foreign_key = t2.id;
```

### 4.3) Normalization and Denormalization Decisions

- [ ] **Eliminar redund√¢ncia desnecess√°ria**
- [ ] **Criar colunas calculadas apenas quando necess√°rio**
- [ ] **Planear n√≠veis de agrega√ß√£o** para an√°lise
- [ ] **Documentar regras de neg√≥cio** e constraints

---

## 5) Data Preparation
**Objetivo:** transformar dados em estruturas limpas e analis√°veis.  
**Entreg√°veis:** datasets limpos, scripts reproduz√≠veis, log de transforma√ß√µes.

### 5.1) Essential Cleaning

- [ ] **Padroniza√ß√£o de formatos:**
  ```python
  # Limpeza b√°sica
  df['col'].str.strip()              # Remover whitespace
  df['col'].str.lower()              # Lowercase
  pd.to_datetime(df['date_col'])     # String para datetime
  pd.to_numeric(df['num_col'], errors='coerce')  # String para num√©rico
  ```

- [ ] **Tratamento de valores em falta:**
  ```python
  # Estrat√©gias por tipo
  df['num_col'].fillna(df['num_col'].median())  # Num√©rico: mediana
  df['cat_col'].fillna('Unknown')               # Categ√≥rico: categoria
  df.fillna(method='ffill')                     # Temporal: forward fill
  ```

### 5.2) Join Operations

- [ ] **Preparar chaves para joins:**
  - Garantir mesmo tipo de dados
  - Validar cobertura entre tabelas
- [ ] **Executar joins com valida√ß√£o:**
  ```python
  # Validar antes/depois do join
  print(f"Before: {len(df1)} records")
  df_merged = df1.merge(df2, on='key', how='left')
  print(f"After: {len(df_merged)} records")
  print(f"Match rate: {df_merged['key_from_df2'].notna().mean():.2%}")
  ```

### 5.3) Basic Feature Engineering

- [ ] **Derivar vari√°veis calculadas:**
  ```python
  # Features temporais
  df['year'] = df['date_col'].dt.year
  df['is_weekend'] = df['date_col'].dt.weekday >= 5
  df['days_since'] = (pd.Timestamp.now() - df['date_col']).dt.days
  
  # Ratios e flags
  df['ratio'] = df['num1'] / df['num2']
  df['is_high_value'] = df['value'] > df['value'].quantile(0.8)
  ```

### 5.4) Validation and Testing

- [ ] **Testes de consist√™ncia p√≥s-limpeza:**
  - Regras de neg√≥cio ainda s√£o v√°lidas?
  - Distribui√ß√µes esperadas s√£o mantidas?
- [ ] **Documentar transforma√ß√µes** com justifica√ß√£o
- [ ] **Guardar vers√£o "gold"** dos dados limpos

---

## 6) Exploratory Data Analysis (EDA)
**Objetivo:** gerar insights e evid√™ncias para decis√µes.  
**Entreg√°veis:** notebook reproduz√≠vel, gr√°ficos explicativos, top insights.

### 6.1) Essential Univariate Analysis

```python
# Vari√°veis num√©ricas
df.describe()                           # Estat√≠sticas descritivas
df.hist(bins=20, figsize=(15, 10))     # Histogramas
df.boxplot(figsize=(15, 5))            # Boxplots para outliers

# Vari√°veis categ√≥ricas
df['col'].value_counts()                # Tabela de frequ√™ncias
df['col'].value_counts(normalize=True)  # Percentagens
df['col'].value_counts().plot.bar()     # Visualiza√ß√£o
```

### 6.2) Key Relationship Analysis

```python
# Correla√ß√µes
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True)

# Num√©rico √ó Categ√≥rico
sns.boxplot(x='category', y='numeric', data=df)
df.groupby('category')['numeric'].describe()

# Categ√≥rico √ó Categ√≥rico
pd.crosstab(df['cat1'], df['cat2'])
pd.crosstab(df['cat1'], df['cat2'], normalize='index')
```

### 6.3) Temporal Analysis (se aplic√°vel)

```python
# Time series
df.set_index('date').resample('M').mean()           # Agrega√ß√£o mensal
df.set_index('date')['value'].rolling(30).mean()    # M√©dia m√≥vel 30 dias

# Padr√µes sazonais
df.groupby(df['date'].dt.dayofweek)['value'].mean() # Por dia da semana
df.groupby(df['date'].dt.month)['value'].mean()     # Por m√™s
```

### 6.4) Segmentation and Insights

- [ ] **An√°lise por quartis/percentis**
- [ ] **Segmentos por l√≥gica de neg√≥cio relevante**
- [ ] **Top 5-7 insights mais importantes** quantificados
- [ ] **Ligar insights** √†s quest√µes de neg√≥cio originais

---

## 7) Insights & Reporting
**Objetivo:** transformar an√°lises em decis√µes e a√ß√µes.  
**Entreg√°veis:** relat√≥rio conciso, gr√°ficos interpretativos, recomenda√ß√µes.

### Report Structure (1-pager)
1. **Contexto & Objetivo**
2. **Dados & Qualidade** (sum√°rio)
3. **3‚Äì5 Main Insights** (cada um com "So what?")
4. **Recomenda√ß√µes e pr√≥ximos passos**
5. **Limita√ß√µes** e ap√™ndices

### ‚úÖ Checklist

- [ ] **Storyline clara:** contexto ‚Üí quest√£o ‚Üí evid√™ncia ‚Üí implica√ß√£o ‚Üí recomenda√ß√£o
- [ ] **Gr√°ficos com t√≠tulos interpretativos** e call-outs
- [ ] **Quantificar impacto esperado** e incerteza
- [ ] **Documentar limita√ß√µes** (bias, qualidade, causalidade)
- [ ] **Package reproduz√≠vel:** notebooks + SQL + README

---

## üõ†Ô∏è **Advanced Toolkit**

<details>
<summary><strong>üìä Database Modeling & Analysis</strong></summary>

```python
# An√°lise autom√°tica de relacionamentos
def analyze_potential_joins(df1, df2):
    """Identifica poss√≠veis colunas de join entre DataFrames"""
    common_cols = set(df1.columns) & set(df2.columns)
    for col in common_cols:
        overlap = len(set(df1[col]) & set(df2[col]))
        print(f"{col}: {overlap} valores comuns")

# Valida√ß√£o de foreign key
def validate_foreign_key(parent_df, child_df, parent_key, foreign_key):
    """Valida integridade referencial"""
    orphans = child_df[~child_df[foreign_key].isin(parent_df[parent_key])]
    print(f"Registos √≥rf√£os: {len(orphans)} ({len(orphans)/len(child_df):.2%})")
    return orphans

# Detec√ß√£o autom√°tica de cardinalidade
def detect_cardinality(df1, df2, key1, key2):
    """Deteta tipo de relacionamento entre tabelas"""
    df1_unique = df1[key1].nunique()
    df2_unique = df2[key2].nunique()
    df1_total = len(df1)
    df2_total = len(df2)
    
    if df1_unique == df1_total and df2_unique == df2_total:
        return "1:1"
    elif df1_unique == df1_total:
        return "1:N"
    elif df2_unique == df2_total:
        return "N:1"
    else:
        return "N:N"
```
</details>

<details>
<summary><strong>‚ö° Performance & Memory Optimization</strong></summary>

```python
# Redu√ß√£o de uso de mem√≥ria
def optimize_dtypes(df):
    """Otimiza tipos de dados para reduzir uso de mem√≥ria"""
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
        else:
            # Converter para category se poucos valores √∫nicos
            if df[col].nunique() / len(df) < 0.5:
                df[col] = df[col].astype('category')
    
    return df

# Processamento de datasets grandes
def process_large_dataset(file_path, chunk_size=10000, processing_func=None):
    """Processa datasets grandes em chunks para evitar erros de mem√≥ria"""
    results = []
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        if processing_func:
            processed_chunk = processing_func(chunk)
        else:
            processed_chunk = chunk
        
        results.append(processed_chunk)
        
        # Monitoriza√ß√£o de mem√≥ria
        import psutil
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > 80:
            print(f"Warning: Memory usage at {memory_percent:.1f}%")
    
    return pd.concat(results, ignore_index=True)
```
</details>

<details>
<summary><strong>üîç Data Quality & Validation</strong></summary>

```python
# Relat√≥rio autom√°tico de qualidade
def data_quality_report(df):
    """Gera relat√≥rio abrangente de qualidade dos dados"""
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),
        'duplicated_rows': df.duplicated().sum(),
        'unique_values': df.nunique().to_dict(),
        'data_types': df.dtypes.to_dict()
    }
    return report

# Valida√ß√£o de regras de neg√≥cio
def validate_business_rules(df, rules):
    """
    Valida regras de neg√≥cio customizadas
    rules = {'age': lambda x: x >= 0, 'email': lambda x: '@' in str(x)}
    """
    violations = {}
    for column, rule in rules.items():
        if column in df.columns:
            violations[column] = ~df[column].apply(rule)
    return violations
```
</details>

<details>
<summary><strong>üìà Advanced EDA Techniques</strong></summary>

```python
# An√°lise avan√ßada de correla√ß√µes
def advanced_correlation_analysis(df):
    """An√°lise de correla√ß√£o com diferentes m√©todos"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    correlations = {
        'pearson': df[numeric_cols].corr(method='pearson'),
        'spearman': df[numeric_cols].corr(method='spearman'),
        'kendall': df[numeric_cols].corr(method='kendall')
    }
    return correlations

# An√°lise de associa√ß√£o categ√≥rica
from scipy.stats import chi2_contingency
def categorical_association(df, col1, col2):
    """Calcula associa√ß√£o entre vari√°veis categ√≥ricas"""
    crosstab = pd.crosstab(df[col1], df[col2])
    chi2, p_value, dof, expected = chi2_contingency(crosstab)
    
    # Cramer's V
    n = crosstab.sum().sum()
    cramers_v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))
    
    return {
        'chi2': chi2,
        'p_value': p_value,
        'cramers_v': cramers_v,
        'significant': p_value < 0.05
    }
```
</details>

---

## üìö **Quick Reference**

| Tipo | Valida√ß√£o | Prepara√ß√£o | EDA | 
|---|---|---|---|
| Num√©rico | intervalos, outliers, unidades | imputa√ß√£o, scaling | boxplot, correla√ß√£o |
| Categ√≥rico | n√≠veis raros, encoding | normalizar, agrupar | top-k, crosstab |
| Datetime | timezone, gaps | features temporais | trend, sazonalidade |
| Texto | encoding, PII | limpeza b√°sica | frequ√™ncias, comprimentos |

## ‚úÖ **Definition of Done por Fase**
- **Setup:** Ambiente configurado e estrutura criada
- **Initial Overview:** Invent√°rio completo e problemas identificados
- **Business Understanding:** KPIs e crit√©rios aprovados
- **Data Understanding:** An√°lise t√©cnica detalhada completada
- **Database Design:** Diagrama ER e estrat√©gia de integra√ß√£o definidos
- **Data Preparation:** Datasets limpos com valida√ß√µes
- **EDA:** Insights priorizados e evid√™ncias quantificadas
- **Reporting:** 1-pager + artefactos para tomada de decis√£o