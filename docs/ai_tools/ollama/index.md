# ğŸ¦™ Ollama Local AI

Setup completo para desenvolvimento com modelos de linguagem locais. Privacy-first development com performance otimizada para macOS.

## ğŸ“‹ **ConteÃºdo DisponÃ­vel**

### [âš¡ Ollama Complete Setup Guide](ollama.md)

**Guia completo para desenvolvimento local com IA**

- âœ… Setup otimizado via Homebrew
- âœ… GestÃ£o avanÃ§ada de modelos (CodeLlama, DeepSeek, Qwen)
- âœ… IntegraÃ§Ã£o completa com VS Code Continue
- âœ… Performance optimization e resource management
- âœ… Privacy e security best practices
- âœ… Troubleshooting e workflows diÃ¡rios

### [ğŸ”§ VS Code Integration Guide](ollama_local_setup.md)

**Setup detalhado para integraÃ§Ã£o VS Code + Continue**

- âœ… VerificaÃ§Ã£o de sistema e dependencies
- âœ… ConfiguraÃ§Ã£o paso-a-passo do Continue
- âœ… AutomaÃ§Ã£o com VS Code tasks
- âœ… Troubleshooting de conexÃµes
- âœ… Model management e optimization

---

## ğŸš€ **Quick Start**

### InstalaÃ§Ã£o BÃ¡sica

```bash
# Install via Homebrew (recommended)
brew install ollama

# Remove conflicting app version
sudo rm -rf /Applications/Ollama.app

# Start as service
brew services start ollama

# Install essential models
ollama pull llama3:8b
ollama pull codellama:34b
ollama pull deepseek-coder:33b
```

### VS Code Integration

1. Install Continue extension in VS Code
2. Configure `~/Library/Application Support/Continue/config.json`
3. Test connection in Continue panel

---

## ğŸ¯ **Casos de Uso Principais**

### Code Development
- **Autocompletion** em tempo real via Continue
- **Code generation** para features complexas
- **Refactoring** e optimization suggestions
- **Code review** e bug detection

### Privacy-First Workflows
- **CÃ³digo proprietÃ¡rio** nunca sai da mÃ¡quina
- **Desenvolvimento offline** completo
- **Zero telemetry** ou data collection
- **Air-gapped environments** suportados

### Model Strategy
- **qwen2.5-coder:1.5b** â†’ Fast autocomplete
- **codellama:34b** â†’ Quality code generation
- **deepseek-coder:33b** â†’ Comprehensive analysis
- **llama3:8b** â†’ General chat e explanation

---

## ğŸ”§ **Terminal Usage**

### Interactive Sessions

```bash
# General chat
ollama run llama3:8b

# Code assistance
ollama run codellama:34b

# Quick coding tasks
ollama run qwen2.5-coder:1.5b
```

### One-shot Queries

```bash
# Code generation
ollama run deepseek-coder "create a REST API with FastAPI"

# Code review
ollama run codellama "optimize this SQL query for performance"

# Explanation
ollama run llama3 "explain this Python function"
```

---

## ğŸ”’ **Privacy & Security**

### Complete Local Control
- **Models stored** em `~/.ollama/models`
- **Zero external API calls** durante inference
- **Complete offline capability**
- **Network isolation** opcional

### Performance Optimization
- **Hardware-aware** model selection
- **Memory management** automÃ¡tico
- **Metal acceleration** em Apple Silicon
- **Resource monitoring** integrado

---

> ğŸ” **Privacy Advantage:** Com Ollama, o teu cÃ³digo proprietÃ¡rio nunca sai da tua mÃ¡quina. Perfeito para projetos sensÃ­veis, trabalho confidencial e ambientes isolados!