{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Diogo Dev Notes","text":"<p>Bem-vindo \u00e0s minhas notas de desenvolvimento. Usa a pesquisa no topo para encontrares comandos/snippets.</p> <p>Adicionado algo.</p> <p>Estrutura</p> <p>A navega\u00e7\u00e3o reflete as subpastas de <code>docs/</code>.</p>"},{"location":"guides/","title":"Templates &amp; Guias de Desenvolvimento","text":"<p>Bem-vindo \u00e0 sec\u00e7\u00e3o de templates da minha dev wiki! Aqui encontras guias organizados e templates para acelerar o desenvolvimento de projetos.</p>"},{"location":"guides/#organizacao","title":"\ud83d\uddc2\ufe0f Organiza\u00e7\u00e3o","text":"<p>Esta sec\u00e7\u00e3o est\u00e1 dividida em categorias l\u00f3gicas para facilitar a navega\u00e7\u00e3o:</p>"},{"location":"guides/#configuracao-inicial","title":"\ud83d\ude80 Configura\u00e7\u00e3o Inicial","text":"<p>Setup essencial do ambiente de desenvolvimento: - Configura\u00e7\u00e3o do MacOS com Homebrew - Setup de ferramentas AI/ML como Ollama</p>"},{"location":"guides/#linguagens-de-programacao","title":"\ud83d\udcbb Linguagens de Programa\u00e7\u00e3o","text":"<p>Guias espec\u00edficos para cada linguagem: - Python: configura\u00e7\u00e3o, UV package manager, workflows completos - Node.js: setup e configura\u00e7\u00e3o do ambiente</p>"},{"location":"guides/#ferramentas-de-desenvolvimento","title":"\ud83d\udee0\ufe0f Ferramentas de Desenvolvimento","text":"<p>Configura\u00e7\u00f5es para ferramentas essenciais: - Git: configura\u00e7\u00e3o completa e melhores pr\u00e1ticas - Claude Code: comandos \u00fateis e diretrizes - ChatGPT: configura\u00e7\u00e3o de MCPs</p>"},{"location":"guides/#analise-de-dados","title":"\ud83d\udcca An\u00e1lise de Dados","text":"<p>Resources completos para projetos de dados: - Metodologias de an\u00e1lise estruturadas - Conceitos de modela\u00e7\u00e3o de dados - Templates SQL prontos a usar</p>"},{"location":"guides/#assets","title":"\ud83c\udfa8 Assets","text":"<p>Recursos partilhados como imagens e ficheiros auxiliares para suporte aos guias.</p>"},{"location":"guides/#como-usar","title":"\ud83c\udfaf Como Usar","text":"<ol> <li>Navega pelas categorias usando o menu lateral</li> <li>Explora cada sec\u00e7\u00e3o para encontrar os templates relevantes</li> <li>Copia e adapta os templates aos teus projetos</li> <li>Contribui com melhorias e novos templates</li> </ol> <p>\ud83d\udca1 Dica: Usa a funcionalidade de pesquisa para encontrar rapidamente o que procuras!</p>"},{"location":"guides/data/","title":"An\u00e1lise de Dados","text":"<p>Recursos, templates e guias para projetos de an\u00e1lise de dados.</p>"},{"location":"guides/data/#data-analysis","title":"\ud83d\udcca Data Analysis","text":"<ul> <li>\ud83d\udcc8 Data Analysis Methodology Guide - Guia metodol\u00f3gico completo</li> <li>\u2705 Data Analysis Checklist - Lista de verifica\u00e7\u00e3o para projetos</li> </ul>"},{"location":"guides/data/#data-modeling","title":"\ud83c\udfd7\ufe0f Data Modeling","text":"<ul> <li>\ud83c\udfd7\ufe0f Data Modeling Concepts - Conceitos fundamentais de modela\u00e7\u00e3o</li> </ul>"},{"location":"guides/data/#sql-templates-resources","title":"\ud83d\udcbe SQL Templates &amp; Resources","text":"<ul> <li>\ud83d\udcca SQL Template - Template base para queries SQL</li> <li>\ud83d\udd17 MySQL Connection Session - Sess\u00e3o de conex\u00e3o MySQL</li> <li>\ud83d\udcdd SQL Queries &amp; Examples - Exemplos e queries \u00fateis</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/","title":"Data modeling","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#conceitos-chave","title":"\ud83d\udccc Conceitos-chave","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#1-entidade","title":"1\ufe0f\u20e3 Entidade","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#o-que-e","title":"O que \u00e9:","text":"<p>\u00c9 um objeto ou conceito do mundo real que queremos representar na base de dados. Uma entidade \u00e9 uma \u201ccoisa\u201d com significado pr\u00f3prio, sobre a qual guardamos informa\u00e7\u00e3o.</p> <p>Exemplos no teu projeto MotoGP:</p> <ul> <li>Piloto</li> <li>Construtor</li> <li>Circuito</li> <li>Corrida</li> <li>Categoria</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/#na-pratica","title":"Na pr\u00e1tica:","text":"<p>No diagrama ER (modelo entidade-relacionamento), uma entidade transforma-se normalmente numa tabela.</p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#analogia","title":"Analogia:","text":"<p>Pensa numa entidade como um \u201csubstantivo\u201d \u2014 algo que existe por si (Piloto, Circuito, Moto).</p> <p></p> <p></p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#2-relacionamento","title":"2\ufe0f\u20e3 Relacionamento","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#o-que-e_1","title":"O que \u00e9:","text":"<p>Descreve como duas ou mais entidades est\u00e3o ligadas entre si.</p> <p>Tipos mais comuns:</p> <ul> <li>1:1 (um para um) \u2192 Um piloto tem exatamente uma data de nascimento.</li> <li>1:N (um para muitos) \u2192 Um construtor pode ter v\u00e1rios pilotos.</li> <li>N:N (muitos para muitos) \u2192 Um piloto participa em muitas corridas, e cada corrida tem muitos pilotos.</li> </ul> <p>Na pr\u00e1tica:</p> <p>Em bases de dados relacionais, rela\u00e7\u00f5es N:N s\u00e3o normalmente resolvidas com uma tabela interm\u00e9dia (junction table).</p> <p>Analogia:</p> <p>Pensa nos relacionamentos como os \u201cverbos\u201d que ligam os substantivos (Piloto participa em Corrida, Circuito est\u00e1 localizado em Pa\u00eds).</p> <p></p> <p></p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#3-dimensao-no-contexto-de-data-warehousing-e-modelos-analiticos","title":"3\ufe0f\u20e3 Dimens\u00e3o (no contexto de Data Warehousing e modelos anal\u00edticos)","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#o-que-e_2","title":"O que \u00e9:","text":"<p>Uma tabela que cont\u00e9m atributos descritivos de uma entidade \u2014 usados para filtrar, agrupar ou segmentar an\u00e1lises.</p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#caracteristicas","title":"Caracter\u00edsticas:","text":"<ul> <li>N\u00e3o armazena medidas num\u00e9ricas \u201cde neg\u00f3cio\u201d que queremossomar/m\u00e9dia.</li> <li>\u00c9 mais est\u00e1tica (n\u00e3o muda tanto no tempo como as tabelasde factos).</li> </ul> <p>Exemplos no MotoGP:</p> <ul> <li>DimRider (informa\u00e7\u00e3o sobre pilotos: nome, pa\u00eds, datanascimento\u2026)</li> <li>DimCircuit (informa\u00e7\u00e3o sobre circuitos: nome, pa\u00eds,comprimento\u2026)</li> <li>DimClass (categorias como MotoGP, Moto2\u2026)</li> <li>DimSeason (ano, n\u00famero de corridas\u2026)</li> </ul> <p>Analogia:</p> <p>Uma dimens\u00e3o \u00e9 como o cart\u00e3o de identidade de algo \u2014 descreve, mas n\u00e3o mede.</p> <p></p> <p></p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#4-tabela-de-facto","title":"4\ufe0f\u20e3 Tabela de Facto","text":""},{"location":"guides/data/Data%20Modeling/data_modeling/#o-que-e_3","title":"O que \u00e9:","text":"<p>Cont\u00e9m eventos/mensura\u00e7\u00f5es de neg\u00f3cio e liga-se \u00e0s dimens\u00f5es atrav\u00e9s de chaves estrangeiras. \u00c9 onde ficam as m\u00e9tricas que vamos analisar.</p> <p>Caracter\u00edsticas:</p> <ul> <li>Cont\u00e9m valores num\u00e9ricos (\u201cmeasures\u201d) que podem ser agregados: soma, m\u00e9dia, contagem\u2026</li> <li>\u00c9 normalmente muito maior que as tabelas de dimens\u00e3o.</li> </ul> <p>Exemplos no MotoGP:</p> <ul> <li>FactRaceResults: posi\u00e7\u00f5es de cada piloto em cada corrida, voltas r\u00e1pidas, pontos.</li> <li>FactConstructorChampionships: t\u00edtulos por construtor e ano.</li> <li>FactSameNationPodiums: eventos de p\u00f3dio com pilotos da mesma na\u00e7\u00e3o.</li> </ul> <p>Analogia:</p> <p>Uma tabela de factos \u00e9 como um livro de registos ou extratos banc\u00e1rios \u2014 cada linha \u00e9 um evento que aconteceu e que podemos medir.</p> <p></p> <p></p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#resumo-visual","title":"\ud83d\udca1 Resumo visual:","text":"<pre><code>DimRider --------\\\nDimConstructor --- FactRaceResults --- DimCircuit\nDimClass --------/                      \\\nDimSeason ------------------------------ DimCountry\n</code></pre> <ul> <li>As dimens\u00f5es est\u00e3o nos lados e descrevem quem, onde, quando, o qu\u00ea.</li> <li>A tabela de factos est\u00e1 no centro e armazena os eventos med\u00edveis.</li> <li>Os relacionamentos ligam tudo com chaves prim\u00e1rias/estrangeiras.</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/#-","title":"---","text":"<p>Boa, bora l\u00e1 \ud83d\udc47</p>"},{"location":"guides/data/Data%20Modeling/data_modeling/#1-entidade_1","title":"1. Entidade","text":"<ul> <li>\u00c9 um objeto ou conceito do mundo real sobre o qual queremos guardar informa\u00e7\u00e3o.</li> <li>Exemplos: <code>Cliente</code>, <code>Produto</code>, <code>Encomenda</code>.</li> <li>Em bases de dados relacionais, cada entidade normalmente transforma-se numa tabela.</li> <li>Cada inst\u00e2ncia da entidade corresponde a uma linha/registo na tabela.</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/#2-relacionamento_1","title":"2. Relacionamento","text":"<ul> <li>Define como as entidades est\u00e3o ligadas entre si.</li> <li>Exemplo: um Cliente faz uma Encomenda \u2192 relacionamento 1:N (um cliente pode ter v\u00e1rias encomendas, mas cada encomenda pertence a um s\u00f3 cliente).</li> <li>Em SQL, isto \u00e9 implementado atrav\u00e9s de chaves estrangeiras (foreign keys).</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/#3-tabelas-de-factos","title":"3. Tabelas de Factos","text":"<ul> <li>Usadas sobretudo em Data Warehousing / BI (modelos estrela ou floco de neve).</li> <li>Guardam os eventos/medidas num\u00e9ricas que queremos analisar.</li> <li>Exemplos: <code>Vendas</code>, <code>Transa\u00e7\u00f5es</code>, <code>Reservas</code>.</li> <li> <p>Cont\u00eam:</p> </li> <li> <p>M\u00e9tricas (ex.: quantidade, pre\u00e7o, valor total)</p> </li> <li>Chaves estrangeiras que ligam \u00e0s tabelas de dimens\u00f5es</li> </ul>"},{"location":"guides/data/Data%20Modeling/data_modeling/#4-tabelas-de-dimensoes-o-outro-tipo-que-estas-a-referir","title":"4. Tabelas de Dimens\u00f5es (o \u201coutro tipo\u201d que est\u00e1s a referir \ud83d\ude09)","text":"<ul> <li>Fornecem contexto \u00e0s tabelas de factos.</li> <li>Exemplos: <code>Clientes</code>, <code>Produtos</code>, <code>Tempo</code>, <code>Localiza\u00e7\u00e3o</code>.</li> <li>Normalmente cont\u00eam atributos descritivos (nome, categoria, pa\u00eds, etc.).</li> <li>Permitem filtrar e agrupar os factos em an\u00e1lises (ex.: \u201cvendas por m\u00eas\u201d, \u201cvendas por pa\u00eds\u201d).</li> </ul> <p>\u26a1 Resumo r\u00e1pido em termos de modela\u00e7\u00e3o para BI:</p> <ul> <li>Entidade: objeto que existe no mundo real.</li> <li>Relacionamento: como os objetos est\u00e3o ligados.</li> <li>Fact table: guarda os acontecimentos quantitativos (medidas).</li> <li>Dimension table: guarda o contexto descritivo para interpretar os factos.</li> </ul> <p>Queres que eu desenhe um mini-exemplo de modelo estrela (com tabelas de factos e dimens\u00f5es) em SQL para visualizares melhor?</p>"},{"location":"guides/data/data_analysis/","title":"Data Analytics Project Methodology Guide &amp; Template","text":"<p>This repository follows a structured methodology for data analytics projects, based on the comprehensive guide/checklist available here. The objective is to ensure reproducibility, clarity, and best practices regardless of data type (numeric, categorical, temporal, text, geospatial, etc.).</p>"},{"location":"guides/data/data_analysis/#project-structure","title":"\ud83d\udcc2 Project Structure","text":"<pre><code>data-analytics-project/\n\u251c\u2500 .venv/\n\u251c\u2500 data/              # raw/, interim/, processed/\n\u2502  \u251c\u2500 raw/            # Original datasets\n\u2502  \u251c\u2500 interim/        # Partially processed data\n\u2502  \u2514\u2500 processed/      # Analysis-ready data\n\u251c\u2500 notebooks/         # 01_xxx.ipynb, 02_xxx.ipynb...\n\u251c\u2500 scripts/           # Standalone processing scripts\n\u251c\u2500 sql/               # 01_exploration.sql, 02_cleaning.sql...\n\u251c\u2500 src/               # Reusable Python modules (ETL, utils, pipelines)\n\u251c\u2500 reports/           # figures/, dashboards/, final reports\n\u2502  \u251c\u2500 figures/\n\u2502  \u2514\u2500 dashboards/\n\u251c\u2500 config/            # .env.example, params.yml\n\u251c\u2500 tests/             # Automated tests\n\u251c\u2500 .env\n\u251c\u2500 .gitignore\n\u251c\u2500 main.py\n\u251c\u2500 pyproject.toml     # Generated by UV\n\u251c\u2500 README.md          # Project description\n\u2514\u2500 LICENSE\n</code></pre>"},{"location":"guides/data/data_analysis/#environment-setup","title":"\ud83d\udee0 Environment Setup","text":"<ol> <li> <p>Clone repository <pre><code>git clone &lt;repo-url&gt;\ncd &lt;repo-name&gt;\n</code></pre></p> </li> <li> <p>Create virtual environment and install dependencies (preferably with UV)    <pre><code># Using UV (recommended)\nuv venv\nsource .venv/bin/activate  # Linux/Mac\n.venv\\Scripts\\activate     # Windows\nuv pip install -r requirements.txt\n\n# Alternative with standard Python\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n.venv\\Scripts\\activate     # Windows\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Configure environment variables <pre><code>cp config/.env.example .env\n# Edit .env as needed\n</code></pre></p> </li> <li> <p>Execute pipeline (example with Makefile)    <pre><code>make run\n</code></pre></p> </li> </ol>"},{"location":"guides/data/data_analysis/#project-methodology","title":"\ud83d\ude80 Project Methodology","text":"<p>This project follows an 8-phase CRISP-DM adapted methodology for Data Analytics:</p>"},{"location":"guides/data/data_analysis/#1-project-setup","title":"1. Project Setup","text":"<p>Objective: Ensure reproducibility, organization, and security from the start. Deliverables: Versioned repository, folder structure, configured environment.</p>"},{"location":"guides/data/data_analysis/#2-initial-data-overview","title":"2. Initial Data Overview","text":"<p>Objective: Rapid inventory of available datasets to understand what we have before formulating business questions. Deliverables: Dataset inventory, initial quality assessment, high-level relationship mapping.</p>"},{"location":"guides/data/data_analysis/#3-business-understanding","title":"3. Business Understanding","text":"<p>Objective: Align problem, expected value, and success criteria based on available data. Deliverables: Problem statement, hypotheses, KPIs, acceptance criteria.</p>"},{"location":"guides/data/data_analysis/#4-data-understanding","title":"4. Data Understanding","text":"<p>Objective: Detailed technical analysis of data quality, structure, and content using Python/SQL tools. Deliverables: Data understanding report, data dictionary, identified problems and opportunities.</p>"},{"location":"guides/data/data_analysis/#5-database-design-integration-planning","title":"5. Database Design &amp; Integration Planning","text":"<p>Objective: Design coherent data structure and integration strategy before final preparation. Deliverables: ER diagram, integration strategy, join plan, data dictionary.</p>"},{"location":"guides/data/data_analysis/#6-data-preparation","title":"6. Data Preparation","text":"<p>Objective: Transform data into clean, analyzable structures. Deliverables: Clean datasets, reproducible scripts, transformation log.</p>"},{"location":"guides/data/data_analysis/#7-exploratory-data-analysis-eda","title":"7. Exploratory Data Analysis (EDA)","text":"<p>Objective: Generate insights and evidence for decisions. Deliverables: Reproducible notebook, explanatory graphics, top insights.</p>"},{"location":"guides/data/data_analysis/#8-insights-reporting","title":"8. Insights &amp; Reporting","text":"<p>Objective: Transform analyses into decisions and actions. Deliverables: Concise report, interpretive graphics, recommendations.</p> <p>For detailed implementation of each phase, see the complete checklist.</p>"},{"location":"guides/data/data_analysis/#essential-data-analysis-toolkit","title":"\ud83d\udcca Essential Data Analysis Toolkit","text":""},{"location":"guides/data/data_analysis/#quick-data-overview","title":"Quick Data Overview","text":"<pre><code># Essential commands for initial assessment\ndf.shape                    # Dimensions (rows, columns)\ndf.info()                   # Data types and memory usage\ndf.head(10), df.tail(10)    # First/last rows\ndf.describe()               # Statistical summary\ndf.isnull().sum()           # Missing values count\ndf.nunique()                # Unique values per column\n</code></pre>"},{"location":"guides/data/data_analysis/#data-quality-assessment","title":"Data Quality Assessment","text":"<pre><code># Missing values analysis\ndf.isnull().mean() * 100    # Missing percentage by column\n\n# Duplicates\ndf.duplicated().sum()       # Total duplicate rows\ndf.duplicated(subset=['key']).sum()  # Duplicates by key\n\n# Data type validation\ndf.dtypes                   # Current data types\npd.to_datetime(df['date_col'])  # Convert to datetime\npd.to_numeric(df['num_col'], errors='coerce')  # Convert to numeric\n</code></pre>"},{"location":"guides/data/data_analysis/#basic-eda-patterns","title":"Basic EDA Patterns","text":"<pre><code># Numerical variables\ndf.hist(bins=20, figsize=(15, 10))     # Distributions\ndf.boxplot(figsize=(15, 5))            # Outlier detection\ndf.corr()                              # Correlation matrix\n\n# Categorical variables\ndf['col'].value_counts()               # Frequency counts\ndf['col'].value_counts(normalize=True) # Percentages\n\n# Relationships\npd.crosstab(df['cat1'], df['cat2'])    # Categorical \u00d7 Categorical\ndf.groupby('category')['numeric'].describe()  # Categorical \u00d7 Numeric\n</code></pre>"},{"location":"guides/data/data_analysis/#advanced-analysis-techniques","title":"\ud83d\udd0d Advanced Analysis Techniques","text":"<p>The project includes advanced techniques for: - Database Modeling &amp; ERD: Automatic relationship analysis and foreign key validation - Performance Optimization: Memory usage reduction and execution time profiling - Data Quality Assurance: Automated quality tests and business rules validation - Advanced EDA: Correlation analysis, categorical associations, automatic segmentation - SQL for Data Modeling: CTEs, window functions, complex join validation</p> <p>See the complete toolkit for implementation details.</p>"},{"location":"guides/data/data_analysis/#quick-reference-by-data-type","title":"\ud83d\udccb Quick Reference by Data Type","text":"Type Validation Preparation EDA Numeric intervals, outliers, units imputation, scaling boxplot, correlation Categorical rare levels, encoding normalize, group top-k, crosstab Datetime timezone, gaps temporal features trend, seasonality Text encoding, PII basic cleaning frequencies, lengths"},{"location":"guides/data/data_analysis/#definition-of-done-by-phase","title":"\ud83c\udfaf Definition of Done by Phase","text":"<ul> <li>Setup: Environment configured and structure created</li> <li>Initial Overview: Complete inventory and problems identified  </li> <li>Business Understanding: KPIs and criteria approved</li> <li>Data Understanding: Detailed technical analysis completed</li> <li>Database Design: ER diagram and integration strategy defined</li> <li>Data Preparation: Clean datasets with validations</li> <li>EDA: Prioritized insights and quantified evidence</li> <li>Reporting: 1-pager + artifacts for decision-making</li> </ul>"},{"location":"guides/data/data_analysis/#contributing-guidelines","title":"\ud83d\udcdd Contributing Guidelines","text":"<ul> <li>Maintain consistent file naming (<code>01_filename.ipynb</code>)</li> <li>Store raw data only in <code>data/raw/</code></li> <li>Never commit sensitive data or PII</li> <li>Follow the 8-phase methodology sequence</li> <li>Document all transformations with reasoning</li> <li>Include validation tests for data quality</li> </ul>"},{"location":"guides/data/data_analysis/#documentation","title":"\ud83d\udcc4 Documentation","text":"<ul> <li>Complete Methodology Checklist - Detailed implementation guide for each phase</li> <li>Advanced Toolkit - Python/SQL code examples for complex analysis</li> </ul>"},{"location":"guides/data/data_analysis/#license","title":"\ud83d\udcdc License","text":"<p>This project is distributed under the MIT License. See LICENSE for more details.</p>"},{"location":"guides/data/data_analysis/checklist/","title":"Data Analytics Project Methodology Guide/Checklist (Python/SQL)","text":"<p>Updated on 2025-08-12 This guide is data-agnostic and focuses on reproducible steps, best practices, and clear decisions for those starting in Data Analytics.</p> <p>Sequential workflow (CRISP-DM adapted for Data Analytics):</p> <ol> <li>Project Setup</li> <li>Initial Data Overview</li> <li>Business Understanding </li> <li>Data Understanding</li> <li>Pre-EDA (Sanity Check)</li> <li>Data Modeling + DB Design &amp; Integration Planning</li> <li>Data Preparation</li> <li>Exploratory Data Analysis (EDA)</li> <li>Insights &amp; Reporting</li> </ol>"},{"location":"guides/data/data_analysis/checklist/#0-project-setup","title":"0) Project Setup","text":"<p>Objective: ensure reproducibility, organization, and security from the start. Deliverables: versioned repository, folder structure, configured environment.</p>"},{"location":"guides/data/data_analysis/checklist/#checklist","title":"Checklist","text":"<ul> <li> <p> Create Git repository and minimal structure:   <pre><code>data-analytics-project/\n\u251c\u2500 .venv/\n\u251c\u2500 data/              # raw/, interim/, processed/\n\u2502  \u251c\u2500 raw/\n\u2502  \u251c\u2500 interim/\n\u2502  \u2514\u2500 processed/\n\u251c\u2500 notebooks/         # 01_xxx.ipynb, 02_xxx.ipynb...\n\u251c\u2500 scripts/\n\u251c\u2500 sql/               # 01_exploration.sql, 02_cleaning.sql...\n\u251c\u2500 src/               # utils.py, pipelines/\n\u251c\u2500 reports/           # figures/, dashboards/\n\u251c\u2500 config/            # .env.example, params.yml\n\u251c\u2500 .env\n\u251c\u2500 .gitignore\n\u251c\u2500 main.py\n\u251c\u2500 pyproject.toml     # generated by UV\n\u251c\u2500 README.md\n\u2514\u2500 LICENSE\n</code></pre></p> </li> <li> <p> Define Python environment (preferably UV) and dependencies.</p> </li> <li> Create <code>.env.example</code> (without secrets) and validate variables.</li> <li> Define conventions: file names, notebook prefixes, commits.</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#1-initial-data-overview","title":"1) Initial Data Overview","text":"<p>Objective: rapid inventory of available datasets to understand what we have before formulating business questions. Deliverables: dataset inventory, initial quality assessment, high-level relationship mapping.</p>"},{"location":"guides/data/data_analysis/checklist/#11-dataset-inventory","title":"1.1) Dataset Inventory","text":"<ul> <li> List all files/tables with:</li> <li>Name, size (rows \u00d7 columns), format</li> <li>Last update date and source/origin</li> <li>Data owner/responsible person</li> </ul> <pre><code># Essential commands for inventory\ndf.shape                    # Dimensions (rows, columns)\ndf.info()                   # Data types and memory usage\ndf.head(10), df.tail(10)    # First/last rows\ndf.columns.tolist()         # Complete column list\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#12-high-level-quality-check","title":"1.2) High-Level Quality Check","text":"<ul> <li> Missing values: <code>df.isnull().sum()</code> and <code>df.isnull().mean() * 100</code></li> <li> Duplicates: <code>df.duplicated().sum()</code> and by business key</li> <li> Cardinality: <code>df.nunique()</code> and uniqueness ratio</li> <li> Data types: validate if correct (<code>df.dtypes</code>)</li> <li> Extreme values: <code>df.describe()</code> to identify obvious issues</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#13-initial-relationship-identification","title":"1.3) Initial Relationship Identification","text":"<ul> <li> Common variables across datasets (same/similar names)</li> <li> Potential join keys (foreign keys)</li> <li> Relationship cardinality (1:1, 1:N, N:N)</li> <li> Identify main dataset (fact table) vs auxiliary (dimensions)</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#2-business-understanding","title":"2) Business Understanding","text":"<p>Objective: align problem, expected value, and success criteria based on available data. Deliverables: problem statement, hypotheses, KPIs, acceptance criteria.</p>"},{"location":"guides/data/data_analysis/checklist/#key-questions","title":"Key Questions","text":"<ul> <li>What business decision needs to be made and by whom?</li> <li>Which KPIs/OKRs are affected? What's the baseline and target?</li> <li>What priority hypotheses should we test?</li> <li>What specific questions do we want to ask the data? For what purpose?</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#checklist_1","title":"Checklist","text":"<ul> <li> Define Problem Statement (1\u20133 clear sentences).</li> <li> Map main KPIs and derived metrics.</li> <li> List hypotheses and expected impact (\u2191/\u2193 KPI).</li> <li> Success/failure criteria and target decision.</li> <li> Formulate specific business questions for the data.</li> <li> Define constraints (time, compliance, minimum quality).</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#3-data-understanding","title":"3) Data Understanding","text":"<p>Objective: detailed technical analysis of data quality, structure, and content using Python/SQL tools. Deliverables: data understanding report, data dictionary, identified problems and opportunities.</p>"},{"location":"guides/data/data_analysis/checklist/#31-metadata-and-data-origin","title":"3.1) Metadata and Data Origin","text":"<ul> <li> Document data sources:</li> <li>Data owner/responsible person</li> <li>Publication/creation date</li> <li>Update frequency (SLA)</li> <li>Data generation process</li> <li>Dependencies and upstream systems</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#32-structure-and-dimensions","title":"3.2) Structure and Dimensions","text":"<ul> <li> Basic dimensional analysis:</li> <li><code>df.shape</code> - How many rows \u00d7 columns?</li> <li><code>df.info()</code> - Data types and memory usage</li> <li><code>df.columns.tolist()</code> - Complete column list</li> <li> Naming validation:</li> <li>Do column titles make sense?</li> <li>Consistent conventions?</li> <li>Need to rename columns?</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#33-data-types-and-formats","title":"3.3) Data Types and Formats","text":"<ul> <li> Check data types (<code>df.dtypes</code>):</li> <li>Numeric: int vs float, adequate precision?</li> <li>Strings: object vs category for optimization?</li> <li>Dates: datetime vs object string?</li> <li>Booleans: bool vs int/string?</li> <li> Validate specific formats:</li> <li>Dates: consistent format, timezone</li> <li>Numbers: decimal separators, units</li> <li>Text: encoding, case sensitivity</li> <li>IDs: pattern, fixed/variable length</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#34-content-analysis","title":"3.4) Content Analysis","text":"<ul> <li> Uniqueness and cardinality per column:</li> <li><code>df.nunique()</code> - How many unique values?</li> <li><code>df.nunique() / len(df)</code> - Uniqueness ratio</li> <li>Identify primary key candidates</li> <li>Identify categorical columns (low cardinality)</li> <li> Statistical classification of each column:</li> <li>Numeric continuous vs discrete</li> <li>Categorical nominal vs ordinal</li> <li>Temporal: point vs interval</li> <li>Identifiers/keys</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#35-data-quality-assessment","title":"3.5) Data Quality Assessment","text":"<ul> <li> Missing values analysis (<code>df.isnull().sum()</code>):</li> <li>Missing pattern: random vs systematic?</li> <li>% missing per column: <code>df.isnull().mean()</code></li> <li>Do missing values represent \"zero\" or \"unknown\"?</li> <li>Decide strategy: remove, impute, or keep as category</li> <li> Preliminary outlier detection:</li> <li><code>df.describe()</code> - suspicious min/max?</li> <li>Impossible values (negative ages, future dates)</li> <li>IQR method: values outside Q1-1.5\u00d7IQR or Q3+1.5\u00d7IQR</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#36-duplicates-and-consistency","title":"3.6) Duplicates and Consistency","text":"<ul> <li> Duplicate analysis:</li> <li><code>df.duplicated().sum()</code> - 100% duplicate rows</li> <li>Duplicates by business key (<code>df.duplicated(subset=['key']).sum()</code>)</li> <li>Investigate if duplicates are legitimate or errors</li> <li> Internal consistency:</li> <li>Do calculated fields match components? (e.g., total = sum(parts))</li> <li>Are logical relationships valid? (e.g., end_date &gt; start_date)</li> <li>Are expected ranges respected?</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#4-database-design-integration-planning","title":"4) Database Design &amp; Integration Planning","text":"<p>Objective: design coherent data structure and integration strategy before final preparation. Deliverables: ER diagram, integration strategy, join plan, data dictionary.</p>"},{"location":"guides/data/data_analysis/checklist/#41-entity-and-attribute-mapping","title":"4.1) Entity and Attribute Mapping","text":"<ul> <li> Map entities and attributes:</li> <li>Identify fact tables (events, metrics) vs dimension tables (references, categories)</li> <li>Document each attribute (name, type, description, source)</li> <li> Define primary and foreign keys:</li> <li>Validate uniqueness: <code>df.groupby(key).size().max() == 1</code></li> <li>Test coverage between tables</li> <li> Model relationships:</li> <li>1:1, 1:N, N:N (use bridge tables if necessary)</li> <li>Expected cardinality</li> <li>Document referential integrity rules</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#42-integration-strategy","title":"4.2) Integration Strategy","text":"<ul> <li> Join planning:</li> <li>Join order (fact table as base)</li> <li>Join types (inner/left/outer) based on business rules</li> <li>Strategy for orphaned records</li> <li> Handle naming conflicts:</li> <li>Columns with same name: use explicit suffixes</li> <li>Conflicting values: define source of truth (SSOT)</li> <li> Create simple ER diagram (dbdiagram.io, draw.io, or Mermaid)</li> </ul> <pre><code>-- Foreign key validation before joins\nSELECT \n    COUNT(*) as total_records,\n    COUNT(t2.id) as matched_records,\n    COUNT(*) - COUNT(t2.id) as orphan_records,\n    ROUND(100.0 * COUNT(t2.id) / COUNT(*), 2) as match_rate\nFROM table1 t1\nLEFT JOIN table2 t2 ON t1.foreign_key = t2.id;\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#43-normalization-and-denormalization-decisions","title":"4.3) Normalization and Denormalization Decisions","text":"<ul> <li> Eliminate unnecessary redundancy</li> <li> Create calculated columns only when necessary</li> <li> Plan aggregation levels for analysis</li> <li> Document business rules and constraints</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#5-data-preparation","title":"5) Data Preparation","text":"<p>Objective: transform data into clean, analyzable structures. Deliverables: clean datasets, reproducible scripts, transformation log.</p>"},{"location":"guides/data/data_analysis/checklist/#51-essential-cleaning","title":"5.1) Essential Cleaning","text":"<ul> <li> <p> Format standardization: <pre><code># Basic cleaning\ndf['col'].str.strip()              # Remove whitespace\ndf['col'].str.lower()              # Lowercase\npd.to_datetime(df['date_col'])     # String to datetime\npd.to_numeric(df['num_col'], errors='coerce')  # String to numeric\n</code></pre></p> </li> <li> <p> Missing values treatment: <pre><code># Strategies by type\ndf['num_col'].fillna(df['num_col'].median())  # Numeric: median\ndf['cat_col'].fillna('Unknown')               # Categorical: category\ndf.fillna(method='ffill')                     # Temporal: forward fill\n</code></pre></p> </li> </ul>"},{"location":"guides/data/data_analysis/checklist/#52-join-operations","title":"5.2) Join Operations","text":"<ul> <li> Prepare keys for joins:</li> <li>Ensure same data types</li> <li>Validate coverage between tables</li> <li> Execute joins with validation: <pre><code># Validate before/after join\nprint(f\"Before: {len(df1)} records\")\ndf_merged = df1.merge(df2, on='key', how='left')\nprint(f\"After: {len(df_merged)} records\")\nprint(f\"Match rate: {df_merged['key_from_df2'].notna().mean():.2%}\")\n</code></pre></li> </ul>"},{"location":"guides/data/data_analysis/checklist/#53-basic-feature-engineering","title":"5.3) Basic Feature Engineering","text":"<ul> <li> Derive calculated variables: <pre><code># Temporal features\ndf['year'] = df['date_col'].dt.year\ndf['is_weekend'] = df['date_col'].dt.weekday &gt;= 5\ndf['days_since'] = (pd.Timestamp.now() - df['date_col']).dt.days\n\n# Ratios and flags\ndf['ratio'] = df['num1'] / df['num2']\ndf['is_high_value'] = df['value'] &gt; df['value'].quantile(0.8)\n</code></pre></li> </ul>"},{"location":"guides/data/data_analysis/checklist/#54-validation-and-testing","title":"5.4) Validation and Testing","text":"<ul> <li> Post-cleaning consistency tests:</li> <li>Are business rules still valid?</li> <li>Are expected distributions maintained?</li> <li> Document transformations with reasoning</li> <li> Save \"gold\" version of clean data</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#6-exploratory-data-analysis-eda","title":"6) Exploratory Data Analysis (EDA)","text":"<p>Objective: generate insights and evidence for decisions. Deliverables: reproducible notebook, explanatory graphics, top insights.</p>"},{"location":"guides/data/data_analysis/checklist/#61-essential-univariate-analysis","title":"6.1) Essential Univariate Analysis","text":"<pre><code># Numerical variables\ndf.describe()                           # Descriptive statistics\ndf.hist(bins=20, figsize=(15, 10))     # Histograms\ndf.boxplot(figsize=(15, 5))            # Boxplots for outliers\n\n# Categorical variables\ndf['col'].value_counts()                # Frequency table\ndf['col'].value_counts(normalize=True)  # Percentages\ndf['col'].value_counts().plot.bar()     # Visualization\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#62-key-relationship-analysis","title":"6.2) Key Relationship Analysis","text":"<pre><code># Correlations\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True)\n\n# Numeric \u00d7 Categorical\nsns.boxplot(x='category', y='numeric', data=df)\ndf.groupby('category')['numeric'].describe()\n\n# Categorical \u00d7 Categorical\npd.crosstab(df['cat1'], df['cat2'])\npd.crosstab(df['cat1'], df['cat2'], normalize='index')\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#63-temporal-analysis-if-applicable","title":"6.3) Temporal Analysis (if applicable)","text":"<pre><code># Time series\ndf.set_index('date').resample('M').mean()           # Monthly aggregation\ndf.set_index('date')['value'].rolling(30).mean()    # 30-day moving average\n\n# Seasonal patterns\ndf.groupby(df['date'].dt.dayofweek)['value'].mean() # By day of week\ndf.groupby(df['date'].dt.month)['value'].mean()     # By month\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#64-segmentation-and-insights","title":"6.4) Segmentation and Insights","text":"<ul> <li> Analysis by quartiles/percentiles</li> <li> Segments by relevant business logic</li> <li> Top 5-7 most important insights quantified</li> <li> Link insights to original business questions</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#7-insights-reporting","title":"7) Insights &amp; Reporting","text":"<p>Objective: transform analyses into decisions and actions. Deliverables: concise report, interpretive graphics, recommendations.</p>"},{"location":"guides/data/data_analysis/checklist/#report-structure-1-pager","title":"Report Structure (1-pager)","text":"<ol> <li>Context &amp; Objective</li> <li>Data &amp; Quality (summary)</li> <li>3\u20135 Main Insights (each with \"So what?\")</li> <li>Recommendations and next steps</li> <li>Limitations and appendices</li> </ol>"},{"location":"guides/data/data_analysis/checklist/#checklist_2","title":"Checklist","text":"<ul> <li> Clear storyline: context \u2192 question \u2192 evidence \u2192 implication \u2192 recommendation</li> <li> Graphics with interpretive titles and call-outs</li> <li> Quantify expected impact and uncertainty</li> <li> Document limitations (bias, quality, causality)</li> <li> Reproducible package: notebooks + SQL + README</li> </ul>"},{"location":"guides/data/data_analysis/checklist/#-","title":"---","text":""},{"location":"guides/data/data_analysis/checklist/#essential-toolkit-advanced-commands","title":"Essential Toolkit - Advanced Commands","text":""},{"location":"guides/data/data_analysis/checklist/#database-modeling-erd","title":"Database Modeling &amp; ERD","text":"<pre><code># Automatic relationship analysis\ndef analyze_potential_joins(df1, df2):\n    \"\"\"Identifies possible join columns between DataFrames\"\"\"\n    common_cols = set(df1.columns) &amp; set(df2.columns)\n    for col in common_cols:\n        overlap = len(set(df1[col]) &amp; set(df2[col]))\n        print(f\"{col}: {overlap} common values\")\n\n# Foreign key validation\ndef validate_foreign_key(parent_df, child_df, parent_key, foreign_key):\n    \"\"\"Validates referential integrity\"\"\"\n    orphans = child_df[~child_df[foreign_key].isin(parent_df[parent_key])]\n    print(f\"Orphaned records: {len(orphans)} ({len(orphans)/len(child_df):.2%})\")\n    return orphans\n\n# Automatic cardinality detection\ndef detect_cardinality(df1, df2, key1, key2):\n    \"\"\"Detects relationship type between tables\"\"\"\n    df1_unique = df1[key1].nunique()\n    df2_unique = df2[key2].nunique()\n    df1_total = len(df1)\n    df2_total = len(df2)\n\n    if df1_unique == df1_total and df2_unique == df2_total:\n        return \"1:1\"\n    elif df1_unique == df1_total:\n        return \"1:N\"\n    elif df2_unique == df2_total:\n        return \"N:1\"\n    else:\n        return \"N:N\"\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#performance-memory-optimization","title":"Performance &amp; Memory Optimization","text":"<pre><code># Memory usage reduction\ndef optimize_dtypes(df):\n    \"\"\"Optimizes data types to reduce memory usage\"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n        else:\n            # Convert to category if few unique values\n            if df[col].nunique() / len(df) &lt; 0.5:\n                df[col] = df[col].astype('category')\n\n    return df\n\n# Performance profiling\nimport time\ndef time_operation(func, *args, **kwargs):\n    \"\"\"Measures execution time of operations\"\"\"\n    start = time.time()\n    result = func(*args, **kwargs)\n    end = time.time()\n    print(f\"Operation executed in {end - start:.2f} seconds\")\n    return result\n\n# Memory usage check\ndef memory_usage(df):\n    \"\"\"Shows detailed memory usage by column\"\"\"\n    mem_usage = df.memory_usage(deep=True)\n    return mem_usage.sort_values(ascending=False)\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#data-validation-quality-assurance","title":"Data Validation &amp; Quality Assurance","text":"<pre><code># Automated quality tests\ndef data_quality_report(df):\n    \"\"\"Generates comprehensive data quality report\"\"\"\n    report = {\n        'total_rows': len(df),\n        'total_columns': len(df.columns),\n        'missing_values': df.isnull().sum().to_dict(),\n        'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),\n        'duplicated_rows': df.duplicated().sum(),\n        'unique_values': df.nunique().to_dict(),\n        'data_types': df.dtypes.to_dict()\n    }\n    return report\n\n# Business rules validation\ndef validate_business_rules(df, rules):\n    \"\"\"\n    Validates custom business rules\n    rules = {'age': lambda x: x &gt;= 0, 'email': lambda x: '@' in str(x)}\n    \"\"\"\n    violations = {}\n    for column, rule in rules.items():\n        if column in df.columns:\n            violations[column] = ~df[column].apply(rule)\n    return violations\n\n# Anomaly detection\nfrom sklearn.ensemble import IsolationForest\ndef detect_anomalies(df, columns, contamination=0.1):\n    \"\"\"Detects anomalies using Isolation Forest\"\"\"\n    iso_forest = IsolationForest(contamination=contamination)\n    anomalies = iso_forest.fit_predict(df[columns])\n    return df[anomalies == -1]\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#advanced-eda-techniques","title":"Advanced EDA Techniques","text":"<pre><code># Advanced correlation analysis\ndef advanced_correlation_analysis(df):\n    \"\"\"Correlation analysis with different methods\"\"\"\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n\n    correlations = {\n        'pearson': df[numeric_cols].corr(method='pearson'),\n        'spearman': df[numeric_cols].corr(method='spearman'),\n        'kendall': df[numeric_cols].corr(method='kendall')\n    }\n    return correlations\n\n# Categorical association analysis\nfrom scipy.stats import chi2_contingency\ndef categorical_association(df, col1, col2):\n    \"\"\"Calculates association between categorical variables\"\"\"\n    crosstab = pd.crosstab(df[col1], df[col2])\n    chi2, p_value, dof, expected = chi2_contingency(crosstab)\n\n    # Cramer's V\n    n = crosstab.sum().sum()\n    cramers_v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))\n\n    return {\n        'chi2': chi2,\n        'p_value': p_value,\n        'cramers_v': cramers_v,\n        'significant': p_value &lt; 0.05\n    }\n\n# Automatic segmentation\nfrom sklearn.cluster import KMeans\ndef auto_segmentation(df, features, n_clusters=5):\n    \"\"\"Automatic segmentation using K-means\"\"\"\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    df['segment'] = kmeans.fit_predict(df[features])\n\n    # Profile each segment\n    segment_profiles = df.groupby('segment')[features].agg(['mean', 'std', 'count'])\n    return df, segment_profiles\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#sql-for-data-modeling","title":"SQL for Data Modeling","text":"<pre><code>-- Common Table Expressions (CTEs) for complex analysis\nWITH customer_metrics AS (\n    SELECT \n        customer_id,\n        COUNT(*) as total_orders,\n        SUM(order_value) as total_spent,\n        AVG(order_value) as avg_order_value,\n        MAX(order_date) as last_order_date\n    FROM orders\n    GROUP BY customer_id\n),\ncustomer_segments AS (\n    SELECT *,\n        CASE \n            WHEN total_spent &gt; 1000 THEN 'High Value'\n            WHEN total_spent &gt; 500 THEN 'Medium Value'\n            ELSE 'Low Value'\n        END as segment\n    FROM customer_metrics\n)\nSELECT segment, COUNT(*) as customers, AVG(total_spent) as avg_spent\nFROM customer_segments\nGROUP BY segment;\n\n-- Essential window functions\nSELECT \n    customer_id,\n    order_date,\n    order_value,\n    -- Ranking\n    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_sequence,\n    RANK() OVER (ORDER BY order_value DESC) as value_rank,\n\n    -- Moving aggregations\n    SUM(order_value) OVER (PARTITION BY customer_id ORDER BY order_date \n                          ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_spent,\n    AVG(order_value) OVER (PARTITION BY customer_id ORDER BY order_date \n                          ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3orders,\n\n    -- Lags and Leads\n    LAG(order_value, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_order_value,\n    LEAD(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date\nFROM orders;\n\n-- Complex join validation\nSELECT \n    'customers' as table_name,\n    COUNT(*) as total_records,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    CASE WHEN COUNT(*) = COUNT(DISTINCT customer_id) THEN 'PK Valid' ELSE 'PK Invalid' END as pk_status\nFROM customers\n\nUNION ALL\n\nSELECT \n    'orders' as table_name,\n    COUNT(*) as total_records,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    CONCAT(COUNT(DISTINCT customer_id), ' of ', (SELECT COUNT(*) FROM customers), ' customers have orders') as fk_coverage\nFROM orders;\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#export-reporting","title":"Export &amp; Reporting","text":"<pre><code># Automatic report generation\ndef generate_data_report(df, output_path='data_report.html'):\n    \"\"\"Generates automatic HTML data report\"\"\"\n    import pandas_profiling\n    profile = pandas_profiling.ProfileReport(df, title='Data Analysis Report')\n    profile.to_file(output_path)\n    print(f\"Report generated: {output_path}\")\n\n# Export to multiple formats\ndef export_analysis_results(df, insights_dict, base_name='analysis'):\n    \"\"\"Exports results to different formats\"\"\"\n    # Excel with multiple sheets\n    with pd.ExcelWriter(f'{base_name}.xlsx') as writer:\n        df.to_excel(writer, sheet_name='Data', index=False)\n        pd.DataFrame(insights_dict.items(), columns=['Insight', 'Value']).to_excel(\n            writer, sheet_name='Insights', index=False)\n\n    # CSV\n    df.to_csv(f'{base_name}_data.csv', index=False)\n\n    # JSON for insights\n    import json\n    with open(f'{base_name}_insights.json', 'w') as f:\n        json.dump(insights_dict, f, indent=2)\n\n# Simple dashboard with Plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef create_summary_dashboard(df, numeric_cols, categorical_cols):\n    \"\"\"Creates simple dashboard with main metrics\"\"\"\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('Correlation', 'Distributions', 'Categorical', 'Temporal'),\n        specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n               [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n    )\n\n    # Correlation heatmap\n    corr_matrix = df[numeric_cols].corr()\n    fig.add_trace(\n        go.Heatmap(z=corr_matrix.values, x=corr_matrix.columns, y=corr_matrix.columns),\n        row=1, col=1\n    )\n\n    # Add other charts...\n    fig.update_layout(height=800, title_text=\"Data Analysis Dashboard\")\n    return fig\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<pre><code># Debugging problematic joins\ndef debug_join_issues(df1, df2, key):\n    \"\"\"Diagnoses common join problems\"\"\"\n    print(\"=== Join Debugging ===\")\n\n    # Check data types\n    print(f\"Key type in df1: {df1[key].dtype}\")\n    print(f\"Key type in df2: {df2[key].dtype}\")\n\n    # Check null values\n    print(f\"Nulls in df1[{key}]: {df1[key].isnull().sum()}\")\n    print(f\"Nulls in df2[{key}]: {df2[key].isnull().sum()}\")\n\n    # Check overlap\n    overlap = set(df1[key]) &amp; set(df2[key])\n    print(f\"Common values: {len(overlap)}\")\n    print(f\"Overlap rate: {len(overlap)/len(set(df1[key])):.2%}\")\n\n    # Show examples of non-matching values\n    df1_only = set(df1[key]) - set(df2[key])\n    df2_only = set(df2[key]) - set(df1[key])\n    print(f\"Only in df1: {list(df1_only)[:5]}\")\n    print(f\"Only in df2: {list(df2_only)[:5]}\")\n\n# Encoding issues resolution\ndef fix_encoding_issues(df, text_columns):\n    \"\"\"Resolves common encoding problems\"\"\"\n    for col in text_columns:\n        if col in df.columns:\n            # Decoding attempt\n            try:\n                df[col] = df[col].str.encode('latin1').str.decode('utf8')\n            except:\n                print(f\"Could not fix encoding for column {col}\")\n    return df\n\n# Large dataset processing\ndef process_large_dataset(file_path, chunk_size=10000, processing_func=None):\n    \"\"\"Processes large datasets in chunks to avoid memory errors\"\"\"\n    results = []\n\n    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n        if processing_func:\n            processed_chunk = processing_func(chunk)\n        else:\n            processed_chunk = chunk\n\n        results.append(processed_chunk)\n\n        # Memory monitoring\n        import psutil\n        memory_percent = psutil.virtual_memory().percent\n        if memory_percent &gt; 80:\n            print(f\"Warning: Memory usage at {memory_percent:.1f}%\")\n\n    return pd.concat(results, ignore_index=True)\n\n# Time series consistency validation\ndef validate_time_series(df, date_col, value_col):\n    \"\"\"Validates consistency in time series\"\"\"\n    issues = []\n\n    # Check temporal duplicates\n    duplicates = df[df.duplicated(subset=[date_col], keep=False)]\n    if len(duplicates) &gt; 0:\n        issues.append(f\"Duplicate dates: {len(duplicates)} records\")\n\n    # Check temporal gaps\n    df_sorted = df.sort_values(date_col)\n    date_diffs = df_sorted[date_col].diff()\n    median_diff = date_diffs.median()\n    large_gaps = date_diffs[date_diffs &gt; median_diff * 3]\n    if len(large_gaps) &gt; 0:\n        issues.append(f\"Large temporal gaps: {len(large_gaps)} occurrences\")\n\n    # Check temporal outliers\n    if df[value_col].dtype in ['int64', 'float64']:\n        Q1, Q3 = df[value_col].quantile([0.25, 0.75])\n        IQR = Q3 - Q1\n        outliers = df[(df[value_col] &lt; Q1 - 1.5*IQR) | (df[value_col] &gt; Q3 + 1.5*IQR)]\n        if len(outliers) &gt; 0:\n            issues.append(f\"Outliers detected: {len(outliers)} values\")\n\n    return issues\n</code></pre>"},{"location":"guides/data/data_analysis/checklist/#quick-reference-by-data-type","title":"Quick Reference by Data Type","text":"Type Validation Preparation EDA Numeric intervals, outliers, units imputation, scaling boxplot, correlation Categorical rare levels, encoding normalize, group top-k, crosstab Datetime timezone, gaps temporal features trend, seasonality Text encoding, PII basic cleaning frequencies, lengths"},{"location":"guides/data/data_analysis/checklist/#definition-of-done-by-phase","title":"Definition of Done by Phase","text":"<ul> <li>Setup: Environment configured and structure created</li> <li>Initial Overview: Complete inventory and problems identified</li> <li>Business Understanding: KPIs and criteria approved</li> <li>Data Understanding: Detailed technical analysis completed</li> <li>Database Design: ER diagram and integration strategy defined</li> <li>Data Preparation: Clean datasets with validations</li> <li>EDA: Prioritized insights and quantified evidence</li> <li>Reporting: 1-pager + artifacts for decision-making</li> </ul>"},{"location":"guides/languages/","title":"Linguagens de Programa\u00e7\u00e3o","text":"<p>Guias e configura\u00e7\u00f5es espec\u00edficas para cada linguagem de programa\u00e7\u00e3o.</p>"},{"location":"guides/languages/#python","title":"\ud83d\udc0d Python","text":"<ul> <li>\ud83d\udc0d Python Configuration - Configura\u00e7\u00e3o geral do ambiente Python</li> <li>\ud83d\udc0d UV Python Project Setup - Setup de projetos com UV package manager  </li> <li>\ud83d\udc0d Python Project Workflow - Fluxo de trabalho completo</li> </ul>"},{"location":"guides/languages/#nodejs","title":"\ud83d\udfe2 Node.js","text":"<ul> <li>\ud83d\udfe2 Node Configuration - Configura\u00e7\u00e3o do ambiente Node.js</li> </ul>"},{"location":"guides/languages/python/python/","title":"Configura\u00e7\u00e3o do Ambiente no macOS","text":""},{"location":"guides/languages/python/python/#0-instalar-o-homebrew-no-macos","title":"0. Instalar o Homebrew no macOS","text":"<p>Abre o terminal e executar o seguinte comando: <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p>"},{"location":"guides/languages/python/python/#1-instalar-o-pyenv-se-ainda-nao-estiver-instalado","title":"1. Instalar o pyenv (se ainda n\u00e3o estiver instalado):","text":"<pre><code>brew update\nbrew install pyenv\n</code></pre> <p>Depois, no teu ~/.zshrc (ou ~/.bashrc), adiciona:</p> <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre> <p>Fecha e reabre o terminal ou faz source ~/.zshrc.</p>"},{"location":"guides/languages/python/python/#2-instalar-a-versao-de-python-que-preferires-por-exemplo-3109","title":"2. Instalar a vers\u00e3o de Python que preferires (por exemplo 3.10.9):","text":"<pre><code>pyenv install 3.10.9\npyenv global 3.10.9  # ou pyenv local 3.10.9 se quiseres para o diret\u00f3rio\n</code></pre> <p>Verifica com python --version ou pyenv version.</p>"},{"location":"guides/languages/python/python/#3-criar-venv-dentro-do-projeto","title":"3. Criar venv dentro do projeto:","text":"<p>Dentro da pasta onde vais ter o projeto, por exemplo MotoGP-Project/, faz:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\n</code></pre> <p>(Se o Python do pyenv estiver configurado como \u201cprincipal\u201d, vai usar a vers\u00e3o que definiste.)</p>"},{"location":"guides/languages/python/python/#4-instalar-bibliotecas-essenciais","title":"4. Instalar bibliotecas essenciais:","text":"<pre><code>pip install pandas numpy jupyter requests sqlalchemy\n</code></pre> <p>Posteriormente, podes adicionar requests, sqlalchemy, etc., conforme fores precisando.</p>"},{"location":"guides/languages/python/python/#5-cria-um-ficheiro-requirementstxt-para-registar-as-libs","title":"5. Cria um ficheiro requirements.txt para registar as libs","text":"<pre><code>pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"guides/languages/python/python_workflow/","title":"\ud83d\udc0d Python Project Setup Template with Git, UV, and GitHub CLI","text":"<p>This guide provides a simple, clean, and reproducible workflow to set up a new Python project with a virtual environment, dependencies, Git integration, and GitHub publishing.</p>"},{"location":"guides/languages/python/python_workflow/#1-create-the-project-directory-and-navigate-into-it","title":"\u2699\ufe0f 1. Create the project directory and navigate into it","text":"<pre><code>mkdir testwow\ncd testwow\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#2-initialize-a-local-git-repository","title":"\ud83d\udd27 2. Initialize a local Git repository","text":"<pre><code>git init\n\n\ngset-dcs\ngh dcs\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#3-set-the-python-version-using-uv","title":"\ud83d\udc0d 3. Set the Python version using UV","text":"<pre><code>uv python install 3.11         # Installs Python 3.11 locally (if not already installed)\nuv init                        # Creates pyproject.toml and base structure\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#4-create-and-activate-the-virtual-environment","title":"\ud83e\uddea 4. Create and activate the virtual environment","text":"<pre><code>uv venv                        # Creates .venv virtual environment\nsource .venv/bin/activate      # Activate it (on Unix/macOS)\n# .venv\\Scripts\\activate     # (On Windows)\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#5-install-core-dependencies","title":"\ud83d\udce6 5. Install core dependencies","text":"<pre><code>uv add numpy pandas matplotlib seaborn\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#6-save-dependencies","title":"\ud83d\udccb 6. Save dependencies","text":"<pre><code>uv lock                         # Lock exact dependency versions (creates uv.lock)\nuv pip freeze &gt; requirements.txt # Generate requirements.txt for pip compatibility\n</code></pre> <p>\ud83d\udca1 This creates both <code>uv.lock</code> (for UV users) and <code>requirements.txt</code> (for pip users)</p>"},{"location":"guides/languages/python/python_workflow/#for-new-users-cloning-this-project","title":"\ud83d\udd04 For new users cloning this project","text":""},{"location":"guides/languages/python/python_workflow/#option-a-with-uv-recommended","title":"\ud83d\ude80 Option A - With UV (recommended)","text":"<pre><code>git clone &lt;repository-url&gt;\ncd &lt;project-name&gt;\nuv sync                         # Install dependencies from pyproject.toml + uv.lock\nsource .venv/bin/activate       # Activate environment (macOS/Linux)\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#option-b-with-traditional-pip","title":"\ud83d\udc0d Option B - With traditional pip","text":"<pre><code>git clone &lt;repository-url&gt;\ncd &lt;project-name&gt;\npython -m venv .venv\nsource .venv/bin/activate       # Activate environment (macOS/Linux)\npip install -r requirements.txt # Install dependencies from requirements.txt\n</code></pre> <p>\u2705 UV users: Use <code>uv sync</code> for exact dependencies from lock file \u2705 Pip users: Use <code>pip install -r requirements.txt</code> for compatibility</p>"},{"location":"guides/languages/python/python_workflow/#7-create-base-project-files","title":"\ud83d\udcc1 7. Create base project files","text":"<pre><code>echo \"# testwow\" &gt; README.md\ntouch main.py\ntouch .gitignore\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#8-generate-a-basic-gitignore","title":"\ud83d\udeab 8. Generate a basic .gitignore","text":"<pre><code>echo -e \".venv/\\n__pycache__/\\n.ipynb_checkpoints/\\n*.pyc\\n.DS_Store\" &gt; .gitignore\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#9-make-the-initial-commit","title":"\ud83d\udcbe 9. Make the initial commit","text":"<pre><code>git add .\ngit commit -m \"Initial commit: Project structure and dependencies\"\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#10-connect-to-the-remote-github-repository-choose-one-of-the-options-below","title":"\u2601\ufe0f 10. Connect to the remote GitHub repository (choose one of the options below)","text":""},{"location":"guides/languages/python/python_workflow/#option-a-create-and-push-to-github-via-github-cli","title":"\ud83d\udd01 OPTION A \u2014 Create and push to GitHub via GitHub CLI","text":"<p>\u26a0\ufe0f Requires you to be authenticated (<code>gh auth login</code>)</p> <pre><code># Fixed name\ngh repo create testwow --private --source=. --remote=origin --push\n\n# OR: Use current folder name as repo name\ngh repo create $(basename \"$PWD\") --private --source=. --remote=origin --push\n</code></pre> <p>\u2705 This command: - Creates the GitHub repo - Adds it as remote <code>origin</code> - Pushes your local commits - Sets <code>main</code> as the upstream branch</p>"},{"location":"guides/languages/python/python_workflow/#option-b-link-to-a-manually-created-github-repository","title":"\ud83e\uddf7 OPTION B \u2014 Link to a manually created GitHub repository","text":"<pre><code>git remote add origin git@github.com:diogo-costa-silva/testwow.git\ngit branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#daily-git-workflow-after-setup","title":"\ud83d\ude80 Daily Git workflow after setup","text":"<pre><code># Pull latest changes (if working collaboratively or from multiple machines)\ngit pull\n\n# Edit your code\ncode main.py\n\n# Check for changes\ngit status\n\n# Stage changes\ngit add .\n\n# Commit changes\ngit commit -m \"Describe your changes\"\n\n# Push to GitHub\ngit push\n</code></pre>"},{"location":"guides/languages/python/python_workflow/#useful-commands-to-check-remote-tracking","title":"\ud83e\uddea Useful commands to check remote tracking","text":"<pre><code>git remote -v       # Check remote URL\ngit branch -vv      # Check if local branch is linked to remote\ngh repo view --web  # Open GitHub repo in browser\n</code></pre>"},{"location":"guides/languages/python/uv/","title":"UV Python Project Setup Guide","text":""},{"location":"guides/languages/python/uv/#new-user-setup","title":"New User Setup","text":""},{"location":"guides/languages/python/uv/#cloning-an-existing-uv-project","title":"\ud83d\ude80 Cloning an existing UV project","text":"<pre><code>git clone &lt;repository-url&gt;\ncd &lt;project-name&gt;\nuv sync                         # Install dependencies from pyproject.toml + uv.lock\nsource .venv/bin/activate       # Activate environment (macOS/Linux)\n</code></pre>"},{"location":"guides/languages/python/uv/#alternative-using-traditional-pip","title":"\ud83d\udc0d Alternative: Using traditional pip","text":"<pre><code>git clone &lt;repository-url&gt;\ncd &lt;project-name&gt;\npython -m venv .venv\nsource .venv/bin/activate       # Activate environment (macOS/Linux)\npip install -r requirements.txt # If requirements.txt is available\n</code></pre> <p>\u2705 UV projects: Use <code>uv sync</code> for exact dependency versions \u2705 Pip compatibility: Use <code>pip install -r requirements.txt</code></p>"},{"location":"guides/languages/python/uv/#installation","title":"Installation","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# or\nbrew install uv\n</code></pre>"},{"location":"guides/languages/python/uv/#project-setup","title":"Project Setup","text":"<pre><code># Create new project\nuv init my-project\ncd my-project\n\n# Initialize existing project\nuv init\n</code></pre>"},{"location":"guides/languages/python/uv/#dependency-management","title":"Dependency Management","text":"<pre><code># Add dependencies\nuv add pandas matplotlib seaborn\nuv add --dev pytest black ruff\n\n# Add from requirements.txt\nuv add -r requirements.txt\n\n# Remove dependency\nuv remove package-name\n\n# Update dependencies\nuv sync\n</code></pre>"},{"location":"guides/languages/python/uv/#virtual-environment","title":"Virtual Environment","text":"<pre><code># Create virtual environment (automatic with uv init)\nuv venv                    # Create .venv in current directory\nuv venv my-env             # Create named environment\n\n# Activate virtual environment\nsource .venv/bin/activate  # Linux/macOS\n.venv\\Scripts\\activate     # Windows\n\n# Run commands in venv without activation\nuv run python script.py\nuv run pytest\nuv run jupyter lab\n</code></pre>"},{"location":"guides/languages/python/uv/#python-version-management","title":"Python Version Management","text":"<pre><code># Install specific Python version\nuv python install 3.11\n\n# Use specific Python version\nuv init --python 3.11\n</code></pre>"},{"location":"guides/languages/python/uv/#essential-commands","title":"Essential Commands","text":"<pre><code># Install all dependencies\nuv sync\n\n# Run script\nuv run python main.py\n\n# Add package and run\nuv run --with requests python script.py\n\n# Lock dependencies\nuv lock\n\n# Export requirements\nuv export --format requirements-txt --output-file requirements.txt\n</code></pre>"},{"location":"guides/languages/python/uv/#project-structure","title":"Project Structure","text":"<pre><code>my-project/\n\u251c\u2500\u2500 pyproject.toml     # Project config and dependencies\n\u251c\u2500\u2500 uv.lock           # Locked dependencies\n\u251c\u2500\u2500 .venv/            # Virtual environment\n\u251c\u2500\u2500 src/              # Source code\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"guides/languages/python/uv/#key-files","title":"Key Files","text":"<ul> <li><code>pyproject.toml</code>: Project metadata and dependencies</li> <li><code>uv.lock</code>: Exact dependency versions (commit this)</li> <li><code>.venv/</code>: Virtual environment (add to .gitignore)</li> </ul>"},{"location":"guides/languages/python/uv/#best-practices","title":"Best Practices","text":"<ul> <li>Always commit <code>pyproject.toml</code> and <code>uv.lock</code></li> <li>Add <code>.venv/</code> to <code>.gitignore</code></li> <li>Use <code>uv sync</code> after cloning</li> <li>Use <code>uv run</code> for consistent execution</li> </ul>"},{"location":"guides/setup/","title":"Configura\u00e7\u00e3o Inicial","text":"<p>Esta sec\u00e7\u00e3o cont\u00e9m guias para configurar o ambiente de desenvolvimento desde o in\u00edcio.</p>"},{"location":"guides/setup/#macos-setup","title":"\ud83c\udf4e MacOS Setup","text":"<ul> <li>\ud83c\udf7a HomeBrew Apps - Lista de aplica\u00e7\u00f5es essenciais para desenvolvimento</li> </ul>"},{"location":"guides/setup/#ai-ml-setup","title":"\ud83e\udd16 AI &amp; ML Setup","text":"<ul> <li>\ud83d\udc11 Ollama AI Setup - Configura\u00e7\u00e3o de modelos de linguagem locais</li> </ul>"},{"location":"guides/setup/ollama/","title":"Complete Guide \u2014 Local Environment with Ollama + Continue (VS Code)","text":"<p>Author: Diogo Silva Last update: 2025-08-11</p>"},{"location":"guides/setup/ollama/#1-initial-checks","title":"1. Initial Checks","text":"<p>Before starting, verify that the system meets the requirements.</p> <pre><code># System and CPU\nsw_vers\nuname -m\nsysctl -n machdep.cpu.brand_string 2&gt;/dev/null || true\n\n# Homebrew\nbrew --version\nbrew doctor\n\n# Node.js and npm (&gt;= 18)\nnode -v\nnpm -v\n\n# VS Code CLI (&gt;= 1.90)\ncode -v\n\n# Ollama location and conflicts\nwhich -a ollama\ntype -a ollama\n\n# Ollama version\nollama --version || true\n\n# Active brew services\nbrew services list | grep -i ollama || true\n\n# LaunchAgents/Daemons\nlaunchctl list | grep -i ollama || true\nls -la ~/Library/LaunchAgents | grep -i ollama || true\nls -la /Library/LaunchDaemons | grep -i ollama || true\n\n# Local API on port 11434\nlsof -iTCP:11434 -sTCP:LISTEN -nP || true\ncurl -s http://localhost:11434/api/tags | jq . 2&gt;/dev/null || curl -s http://localhost:11434/api/tags\n\n# Typical directories\necho \"BREW prefix: $(brew --prefix)\"\nls -la \"$HOME/.ollama\" || true\nls -la \"$HOME/Library/Application Support/Ollama\" || true\n\n# Continue directory\nls -la \"$HOME/Library/Application Support/Continue\" || true\n</code></pre>"},{"location":"guides/setup/ollama/#2-installing-and-configuring-ollama-brew","title":"2. Installing and Configuring Ollama (brew)","text":"<pre><code># Install via brew\nbrew install ollama\n\n# Remove App (if exists)\nsudo rm -rf /Applications/Ollama.app\n\n# Stop and clean old services\nbrew services stop ollama || true\nlaunchctl bootout gui/$(id -u) \"$HOME/Library/LaunchAgents/homebrew.mxcl.ollama.plist\" 2&gt;/dev/null || true\nsudo launchctl bootout system /Library/LaunchDaemons/homebrew.mxcl.ollama.plist 2&gt;/dev/null || true\nsudo rm -f /Library/LaunchDaemons/homebrew.mxcl.ollama.plist\n\n# Start service on startup\nbrew services start ollama\n\n# Start ollama in terminal\nollama serve\n\n# Check service\nbrew services list | grep -i ollama\nlsof -iTCP:11434 -sTCP:LISTEN -nP\ncurl -s http://localhost:11434/api/version\n</code></pre>"},{"location":"guides/setup/ollama/#3-managing-models-in-ollama","title":"3. Managing Models in Ollama","text":""},{"location":"guides/setup/ollama/#31-installing-models","title":"3.1 Installing Models","text":"<pre><code># Main model\nollama pull llama3:8b\n\n# Coding Models\nollama pull codellama:34b\nollama pull deepseek-coder:33b\n\n\n\n\n\n# Other useful models\nollama pull codellama\nollama pull mistral\nollama pull phi3\nollama pull qwen2.5-coder:1.5b-base\nollama pull nomic-embed-text:latest\n</code></pre>"},{"location":"guides/setup/ollama/#32-list-models","title":"3.2 List Models","text":"<pre><code>ollama list\n</code></pre>"},{"location":"guides/setup/ollama/#33-remove-models","title":"3.3 Remove Models","text":"<pre><code>ollama rm model_name\n</code></pre>"},{"location":"guides/setup/ollama/#34-view-model-details","title":"3.4 View Model Details","text":"<pre><code>ollama show model_name\n</code></pre>"},{"location":"guides/setup/ollama/#4-using-ollama-in-terminal","title":"4. Using Ollama in Terminal","text":"<pre><code># Interactive Chat\nollama run llama3\n\n# Code\nollama run codellama\n\n# Autocomplete\nollama run qwen2.5-coder:1.5b\n\n# Embeddings\nollama run nomic-embed-text:latest\n\n# Exit\nCtrl + C\n</code></pre>"},{"location":"guides/setup/ollama/#5-integration-with-continue-vs-code","title":"5. Integration with Continue (VS Code)","text":""},{"location":"guides/setup/ollama/#51-install-extension","title":"5.1 Install Extension","text":"<ul> <li>VS Code \u2192 Extensions \u2192 Search for <code>Continue</code> \u2192 Install.</li> </ul>"},{"location":"guides/setup/ollama/#52-configuration","title":"5.2 Configuration","text":"<p>Create/edit: <pre><code>~/Library/Application Support/Continue/config.json\n</code></pre> Example content: <pre><code>{\n  \"models\": [\n    {\n      \"provider\": \"ollama\",\n      \"apiBase\": \"http://localhost:11434\",\n      \"model\": \"llama3:8b\",\n      \"roles\": [\"chat\", \"autocomplete\", \"edit\"]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"guides/setup/ollama/#53-test","title":"5.3 Test","text":"<ul> <li>Open Continue panel (<code>\u2318J</code> \u2192 choose Continue).</li> <li>Execute: <pre><code>/model llama3:8b\n</code></pre></li> <li>If it responds, connection is OK.</li> </ul>"},{"location":"guides/setup/ollama/#6-vs-code-automation","title":"6. VS Code Automation","text":"<p><code>.vscode/tasks.json</code>: <pre><code>{\n  \"version\": \"2.0.0\",\n  \"tasks\": [\n    {\n      \"label\": \"Start Ollama\",\n      \"type\": \"shell\",\n      \"command\": \"brew services start ollama\",\n      \"problemMatcher\": []\n    },\n    {\n      \"label\": \"Stop Ollama\",\n      \"type\": \"shell\",\n      \"command\": \"brew services stop ollama\",\n      \"problemMatcher\": []\n    }\n  ]\n}\n</code></pre></p>"},{"location":"guides/setup/ollama/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Use <code>.env</code> for tokens/credentials and load with <code>export $(cat .env | xargs)</code></li> <li>Don't commit <code>config.json</code> with private keys.</li> <li>Monitor space: <pre><code>du -sh ~/.ollama/models\n</code></pre></li> </ul>"},{"location":"guides/setup/ollama/#8-troubleshooting","title":"8. Troubleshooting","text":"<pre><code># Ollama Logs\nlog show --predicate 'process == \"ollama\"' --last 10m\n\n# Foreground startup\nOLLAMA_LOG=debug ollama serve\n\n# Port in use\nlsof -iTCP:11434 -sTCP:LISTEN -nP\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/","title":"Homebrew Apps - macOS Setup","text":""},{"location":"guides/setup/homebrew/homebrew/#install-homebrew","title":"Install Homebrew","text":"<pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#essential-tools","title":"Essential Tools","text":""},{"location":"guides/setup/homebrew/homebrew/#mac-app-store-cli","title":"Mac App Store CLI","text":"<pre><code>brew install mas\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#development-tools","title":"Development Tools","text":""},{"location":"guides/setup/homebrew/homebrew/#terminal-code-editors","title":"Terminal &amp; Code Editors","text":"<pre><code># iTerm2 - Better terminal\nbrew install --cask iterm2\n\n# VS Code - Code editor\nbrew install --cask visual-studio-code\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#development-utilities","title":"Development Utilities","text":"<pre><code># Android Platform Tools (ADB)\nbrew install android-platform-tools\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#internet-communication","title":"Internet &amp; Communication","text":""},{"location":"guides/setup/homebrew/homebrew/#web-browsers","title":"Web Browsers","text":"<pre><code># Arc Browser\nbrew install --cask arc\n\n# Google Chrome\nbrew install --cask google-chrome\n\n# Firefox\nbrew install --cask firefox\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#ai-communication","title":"AI &amp; Communication","text":"<pre><code># ChatGPT\nbrew install --cask chatgpt\n\n# DeepL Translator\nbrew install --cask deepl\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#productivity-utilities","title":"Productivity &amp; Utilities","text":""},{"location":"guides/setup/homebrew/homebrew/#file-management-storage","title":"File Management &amp; Storage","text":"<pre><code># Google Drive\nbrew install --cask google-drive\n\n# MacDroid - Android file transfer\nbrew install --cask macdroid\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#productivity-apps","title":"Productivity Apps","text":"<pre><code># Notion - Notes and workspace\nbrew install --cask notion\n\n# qBittorrent - Torrent client\nbrew install --cask qbittorrent\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#security","title":"Security","text":"<pre><code># KeePassXC - Password manager\nbrew install --cask keepassxc\n</code></pre>"},{"location":"guides/setup/homebrew/homebrew/#accessibility-tools","title":"Accessibility Tools","text":"<pre><code># UnnaturalScrollWheels - Scroll direction control\nbrew install --cask unnaturalscrollwheels\n\n# Amphetamine - Keep Mac awake (from Mac App Store)\nmas install 937984704\n</code></pre>"},{"location":"guides/tools/","title":"Ferramentas de Desenvolvimento","text":"<p>Configura\u00e7\u00f5es e guias para ferramentas essenciais de desenvolvimento.</p>"},{"location":"guides/tools/#controlo-de-versao","title":"\ud83d\udcda Controlo de Vers\u00e3o","text":"<ul> <li>\ud83d\udcda Git Configuration - Configura\u00e7\u00e3o completa do Git</li> </ul>"},{"location":"guides/tools/#claude-code","title":"\ud83e\udd16 Claude Code","text":"<ul> <li>\u2328\ufe0f Claude Code Commands - Lista de comandos \u00fateis</li> <li>\ud83d\udccb Claude Code Guidelines - Melhores pr\u00e1ticas e diretrizes</li> <li>\ud83d\uddc4\ufe0f MySQL MCP Setup - Configura\u00e7\u00e3o do MySQL MCP</li> </ul>"},{"location":"guides/tools/#chatgpt","title":"\ud83d\udcac ChatGPT","text":"<ul> <li>\ud83d\udd0c MCPs GPT - Configura\u00e7\u00e3o de MCPs para ChatGPT</li> </ul>"},{"location":"guides/tools/Claude%20Code/cc_commands/","title":"Claude Code Commands","text":"<pre><code>claude\n</code></pre> <p>shift + tab = plan mode</p> <pre><code># inside claude code in terminal\n\n/mcp                      # ver servidores registados\n/status                   # ver contexto\n</code></pre> <pre><code>claude mcp list\n\nclaude mcp remove &lt;mcp-name&gt;\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/","title":"Claude Code Guidelines","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#1-claude-rules","title":"1. Claude Rules","text":"<ol> <li> <p>First think through the problem, read the codebase for relevant files, and write a plan to tasks/todo.md.</p> </li> <li> <p>The plan should have a list of todo items that you can check off as you complete them</p> </li> <li> <p>Before you begin working, check in with me and I will verify the plan.</p> </li> <li> <p>Then, begin working on the todo items, marking them as complete as you go.</p> </li> <li> <p>Please every step of the way just give me a high level explanation of what changes you made</p> </li> <li> <p>Make every task and code change you do as simple as possible. We want to avoid making any massive or complex changes. Every change should impact as little code as possible. Everything is about simplicity.</p> </li> <li> <p>Finally, add a review section to the todo.md file with a summary of the changes you made and any other relevant information.</p> </li> <li> <p>All resulting products (code, documentation, comments, variable names, etc.) must be written in English.</p> </li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#2-use-plan-mode","title":"2. Use Plan Mode","text":"<pre><code>open claude\n# Press Shift + Tab (2x) to enter Plan Mode\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/#best-practices","title":"Best Practices:","text":"<ol> <li>Over use Plan Mode - Better to plan too much than too little</li> <li>Use the right model for each phase:</li> <li>Opus \u2192 Planning and complex reasoning      <pre><code>/model opus\n</code></pre></li> <li>Sonnet \u2192 Code execution and implementation      <pre><code>/model sonnet\n</code></pre></li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#3-set-claude-code-to-live-dangerously-by-bypassing-unnecessary-permissions","title":"3. Set Claude Code to \"live dangerously\" by bypassing unnecessary permissions","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#why-use-this-flag","title":"Why use this flag:","text":"<ul> <li>Faster execution - Skip permission prompts for common operations</li> <li>Better workflow - Reduce interruptions during development</li> <li>Trust your setup - When working in safe, controlled environments</li> </ul> <pre><code># Add to your shell profile (~/.bashrc, ~/.zshrc, etc.)\nalias claude='claude --dangerously-skip-permissions'\n</code></pre> <p>\u26a0\ufe0f Warning: Only use in trusted development environments!</p>"},{"location":"guides/tools/Claude%20Code/cc_rules/#4-saving-savepoints","title":"4. Saving Savepoints","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#strategy","title":"Strategy:","text":"<ul> <li>Use Git commits for regular savepoints</li> <li>Create branches for experimental features</li> <li>Push frequently to avoid losing work</li> </ul> <pre><code>git add .\ngit commit -m \"Save progress: feature description\"\ngit push\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/#5-when-to-use-images","title":"5. When to use images","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#effective-use-cases","title":"Effective use cases:","text":"<ol> <li>UI Inspiration - Show mockups or design references</li> <li>Bug fixing - Screenshots of error messages or UI issues</li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#6-use-clear","title":"6. Use <code>/clear</code>","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#benefits","title":"Benefits:","text":"<ol> <li>Reduce hallucinations - Fresh context prevents confusion</li> <li>Save money - Smaller context windows cost less</li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#7-security-checks","title":"7. Security Checks","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#workflow","title":"Workflow:","text":"<ol> <li>Use plan mode - Plan your security approach</li> <li>Execute - Implement the planned changes</li> <li>Security review - Run comprehensive security checks</li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#security-review-prompt","title":"Security Review Prompt:","text":"<pre><code>Please check through all the code you just wrote and make sure it follows security best practices. Make sure there are no sensitive information in the frontend and there are no vulnerabilities that can be exploited.\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/#8-learn-from-claude","title":"8. Learn from Claude","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#learning-workflow","title":"Learning Workflow:","text":"<ol> <li>Use plan mode - Plan your implementation approach</li> <li>Execute - Build the feature or fix</li> <li>Security checks - Ensure code safety</li> <li>Learn &amp; understand - Deep dive into what was built</li> </ol>"},{"location":"guides/tools/Claude%20Code/cc_rules/#learning-prompt","title":"Learning Prompt:","text":"<pre><code>Please explain the functionality and code you just built out in detail. Walk me through what you changed and how it works. Act like you're a senior engineer teaching me code.\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/#9-be-productive-while-claude-cooks","title":"9. Be productive while Claude cooks","text":""},{"location":"guides/tools/Claude%20Code/cc_rules/#productive-use-of-wait-time","title":"Productive Use of Wait Time:","text":"<p>Instead of doom scrolling during AI processing time, use these strategies:</p>"},{"location":"guides/tools/Claude%20Code/cc_rules/#prompt-for-productive-conversations","title":"Prompt for Productive Conversations:","text":"<pre><code>When I am coding with AI there are long breaks between me giving commands to the AI. Typically I spend that time doom scrolling which distracts me and puts me in a bad mental state. I'd like to use that time now to chat with you and generate new ideas, and also reflect on my other ideas and businesses and content. I'm not sure how I'd like to use this chat or what role I'd like you to play, but I think it could be much more useful than me doom scrolling. What do you think? What could be the best way for us to use this chat?\n</code></pre>"},{"location":"guides/tools/Claude%20Code/cc_rules/#alternative-activities","title":"Alternative Activities:","text":"<ul> <li>Code review - Review what Claude just implemented</li> <li>Architecture planning - Think about next features</li> <li>Documentation - Update README or comments</li> <li>Testing strategy - Plan test cases</li> <li>Refactoring opportunities - Identify areas for improvement</li> </ul>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/","title":"\u2328\ufe0f Claude Code + MySQL MCP Server no MacOS","text":"<p>Este guia explica, passo a passo, como ligar o Claude Code diretamente a bases de dados MySQL locais no macOS, usando o mysql_mcp_server da designcomputer.</p> <p>O resultado final: vais poder abrir o Claude Code, escrever queries SQL (<code>SHOW DATABASES;</code>, <code>SELECT ...</code>) e o Claude responde com resultados reais da tua inst\u00e2ncia MySQL. \ud83e\udd2f</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#1-pre-requisitos","title":"1. Pr\u00e9-requisitos","text":"<p>No macOS, garante que tens:</p> <ul> <li>Claude Code instalado: Anthropic Claude Code</li> <li>MySQL a correr localmente (<code>brew install mysql</code> ou Docker).</li> <li>Python 3.11+ (usa <code>uv</code> para gerir ambientes virtuais).</li> <li>uv instalado: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></li> </ul>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#2-criar-um-ambiente-uv-isolado-para-o-mcp","title":"2. Criar um ambiente UV isolado para o MCP","text":"<pre><code>mkdir -p ~/.mcp-envs/mysql\ncd ~/.mcp-envs/mysql\n\nuv venv\nsource .venv/bin/activate\n</code></pre> <p>Ativas o venv s\u00f3 para instalar/atualizar o servidor MCP.</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#21-garantir-pip-no-venv-as-vezes-falta-no-uv","title":"2.1. Garantir pip no venv (\u00e0s vezes falta no uv)","text":"<pre><code>python -m ensurepip --upgrade\npython -m pip install --upgrade pip\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#3-instalar-o-servidor-mysql-mcp","title":"3. Instalar o servidor MySQL MCP","text":"<pre><code># Instalar via reposit\u00f3rio oficial:\npip install git+https://github.com/designcomputer/mysql_mcp_server.git\n\n# Instalar via pip:\npip install mysql-mcp-server\n</code></pre> <p>Confirma que ficou instalado:</p> <pre><code>~/.mcp-envs/mysql/.venv/bin/mysql_mcp_server --help\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#4-configuracao-de-credenciais-segura","title":"4. Configura\u00e7\u00e3o de credenciais (segura)","text":""},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#opcao-a-usar-env-file-mais-simples","title":"Op\u00e7\u00e3o A) Usar .env file (mais simples)","text":"<p>Cria um ficheiro ~/.mcp-envs/mysql/.env:</p> <pre><code>cat &gt; ~/.mcp-envs/mysql/.env &lt;&lt;'ENV'\nMYSQL_HOST=127.0.0.1\nMYSQL_PORT=3306\nMYSQL_USER=root\nMYSQL_PASSWORD=&lt;&lt;&lt;colocar_s\u00f3_aqui&gt;&gt;&gt;\n# Use uma BD neutra por defeito (este servidor exige uma):\nMYSQL_DATABASE=information_schema\nENV\n</code></pre> <p>Nota: mesmo com MYSQL_DATABASE=information_schema, podes fazer USE outra_bd; dentro do Claude.</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#opcao-b-guardar-password-no-keychain-do-macos","title":"Op\u00e7\u00e3o B) Guardar password no Keychain do macOS","text":"<p>Mais seguro: guarda a password uma vez no Keychain:</p> <pre><code>security add-generic-password -a \"$USER\" -s mysql_root -w 'podimon10'\n</code></pre> <p>Depois edita <code>~/.mcp-envs/mysql/.env</code> sem password:</p> <pre><code>MYSQL_HOST=127.0.0.1\nMYSQL_PORT=3306\nMYSQL_USER=root\n# deixa a password vazia para vir do Keychain\nMYSQL_PASSWORD=\n# Use uma BD neutra por defeito (este servidor exige uma):\nMYSQL_DATABASE=information_schema\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#5-criar-wrapper-para-o-servidor-mcp","title":"5. Criar wrapper para o servidor MCP","text":"<p>Cria <code>~/bin/mysql-mcp-env.sh</code>: <code>touch ~/bin/mysql-mcp-env.sh</code></p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nLOG_TAG=\"[mysql-mcp-env]\"\n\n# 1) Carregar .env se existir\nif [ -f \"$HOME/.mcp-envs/mysql/.env\" ]; then\n  set -a\n  . \"$HOME/.mcp-envs/mysql/.env\"\n  set +a\nelse\n  echo \"$LOG_TAG Aviso: ~/.mcp-envs/mysql/.env n\u00e3o encontrado\" &gt;&amp;2\nfi\n\n# 2) Fallback: password do Keychain se n\u00e3o houver MYSQL_PASSWORD\nif [ \"${MYSQL_PASSWORD:-}\" = \"\" ]; then\n  if PW=\"$(security find-generic-password -a \"$USER\" -s mysql_root -w 2&gt;/dev/null)\"; then\n    export MYSQL_PASSWORD=\"$PW\"\n  else\n    echo \"$LOG_TAG ERRO: Sem MYSQL_PASSWORD e sem entrada no Keychain (service=mysql_root)\" &gt;&amp;2\n    exit 1\n  fi\nfi\n\n# 3) Obrigat\u00f3rios: host, user, database\n: \"${MYSQL_HOST:=127.0.0.1}\"\n: \"${MYSQL_PORT:=3306}\"\n: \"${MYSQL_USER:?Falta MYSQL_USER}\"\n: \"${MYSQL_DATABASE:?Falta MYSQL_DATABASE (usa p.ex. information_schema)}\"\n\n# 4) export expl\u00edcito\nexport MYSQL_HOST MYSQL_PORT MYSQL_USER MYSQL_PASSWORD MYSQL_DATABASE\n\n# 5) Arrancar o server do venv\nEXEC=\"$HOME/.mcp-envs/mysql/.venv/bin/mysql_mcp_server\"\nif [ ! -x \"$EXEC\" ]; then\n  echo \"$LOG_TAG ERRO: Bin\u00e1rio n\u00e3o encontrado em $EXEC\" &gt;&amp;2\n  exit 1\nfi\n\nexec \"$EXEC\"\n</code></pre> <p>Torna-o execut\u00e1vel:</p> <pre><code>chmod +x ~/bin/mysql-mcp-env.sh\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#6-registar-o-mcp-no-claude-code","title":"6. Registar o MCP no Claude Code","text":"<p>Remove configs antigas (se existirem):</p> <pre><code>claude mcp remove mysql_rw_films -s user 2&gt;/dev/null || true\n</code></pre> <p>Adiciona o novo server, apontando para o wrapper:</p> <pre><code>claude mcp add --scope user mysql_multi_rw -- ~/bin/mysql-mcp-env.sh\n</code></pre> <p>Confirma:</p> <pre><code>claude mcp list\nclaude mcp get mysql_multi_rw\n</code></pre> <p>Deve aparecer \u2705 Connected.</p> <p>\u2e3b</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#7-usar-no-claude-code","title":"7. Usar no Claude Code \ud83d\ude80","text":"<p>Abre o Claude Code:</p> <pre><code>claude\n</code></pre> <p>No REPL:</p> <pre><code>SHOW DATABASES;\nUSE films;\nSHOW TABLES;\nSELECT COUNT(*) FROM sqlzoo.game;\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#8-boas-praticas-de-seguranca","title":"8. Boas pr\u00e1ticas de seguran\u00e7a","text":"<p>\u2022   Nunca commits o .env \u2192 adiciona ao .gitignore.</p> <p>\u2022   Usa o Keychain (security add-generic-password) para n\u00e3o ter passwords em ficheiro.</p> <p>\u2022   Cria um utilizador MySQL s\u00f3 para o Claude, com permiss\u00f5es limitadas:</p> <pre><code>CREATE USER 'claude'@'localhost' IDENTIFIED BY 'senha_segura';\nGRANT SELECT, INSERT, UPDATE, DELETE ON *.* TO 'claude'@'localhost';\nFLUSH PRIVILEGES;\n</code></pre> <p>\u2022   Usa information_schema como DB default neutra \u2192 troca com USE quando precisares.</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#9-troubleshooting","title":"9. Troubleshooting","text":"<p>\u2022   claude mcp list mostra \u2717? Testa:</p> <pre><code>mysql -uroot -ppassword -h127.0.0.1 -P3306 -e \"SELECT 1;\"\n</code></pre> <p>\u2022   Se o Claude pedir sempre \u201ctrust folder\u201d, garante que o corres na tua pasta de projeto (n\u00e3o dentro de ~/.mcp-envs).</p> <p>\u2022   Ver se o wrapper est\u00e1 a exportar as vari\u00e1veis:</p> <pre><code>bash -x ~/bin/mysql-mcp-env.sh\n</code></pre> <p>Se der \u201cMissing required database configuration\u201d, \u00e9 porque faltam vari\u00e1veis no .env ou o Keychain n\u00e3o devolveu nada.</p>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#10-conteudo-a-adicionar-ao-gitignore","title":"10. Conteudo a adicionar ao <code>gitignore</code>","text":"<pre><code># MCP local setup\n.mcp-envs/\n*.env\n</code></pre>"},{"location":"guides/tools/Claude%20Code/mysql_mcp_setup/#11-atualizar-a-biblioteca","title":"11. Atualizar a biblioteca","text":"<pre><code>source ~/.mcp-envs/mysql/.venv/bin/activate &amp;&amp; pip install -U mysql_mcp_server\n</code></pre>"},{"location":"guides/tools/git/git/","title":"Git","text":""},{"location":"guides/tools/git/git/#colocar-projetos-no-github","title":"Colocar projetos no GitHub","text":"<pre><code># Initial setup\ngset-dcs\ngh dcs\n\n# Add &amp; Commit \ngit add .\ngit commit -m \"Initial commit\"\n\n# Create GitHub repository\ngh repo create motogp-analytics --public --source=. --push\n\n# Push to GitHub\ngit push origin master\n\n# \ngit remote set-url origin git@github.com:diogo-costa-silva/mac-setup.git\n</code></pre>"}]}